{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29ab8dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "from scipy.optimize import lsq_linear\n",
    "from rw_data_processing import *\n",
    "from Data_synthesize import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.style.use(\"./rw_visualization.mplstyle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4888a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Color\n",
    "current_palette = sns.color_palette()\n",
    "current_palette\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af12a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_p_state = True # Set False if you do not want to save any parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f43758",
   "metadata": {},
   "source": [
    "# 1. Demographic parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e3108e",
   "metadata": {},
   "source": [
    "## 1.1 Household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53153647",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_data_path = Path('./data/demographic_data')\n",
    "family_size_data = pd.read_excel(demographic_data_path/'縣市戶數結構.xls', sheet_name='110')\n",
    "family_size_data.columns = family_size_data.iloc[1]\n",
    "family_size_data = family_size_data[3:23+1]\n",
    "family_size_data = family_size_data[family_size_data['區  域  別'] != '臺  灣  省']\n",
    "family_size_data = family_size_data[family_size_data['區  域  別'] != '福  建  省']\n",
    "family_size_data = family_size_data[family_size_data['區  域  別'] != '澎  湖  縣']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6d9e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load total number of famaliy each city\n",
    "family_number_data = pd.read_excel(demographic_data_path/'縣市村里鄰戶數及人口數-110年.xls')\n",
    "family_number_data.columns = family_number_data.iloc[1]\n",
    "family_number_data = family_number_data[4:28]\n",
    "family_number_data = family_number_data[family_number_data['區域別'] != '臺灣省']\n",
    "family_number_data = family_number_data[family_number_data['區域別'] != '福建省']\n",
    "family_number_data = family_number_data[family_number_data['區域別'] != '澎湖縣']\n",
    "family_number_data = family_number_data[family_number_data['區域別'] != '連江縣']\n",
    "family_number_data = family_number_data[family_number_data['區域別'] != '金門縣']\n",
    "family_number = family_number_data['人口數'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e07863d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the household cases are too less compare to Cheng et al. even when the household contact p close to 1. I thus adjust this probability.\n",
    "family_size_dict = {}\n",
    "for i, city in enumerate(family_size_data['區  域  別']):\n",
    "    family_size_array = family_size_data[family_size_data['區  域  別'] == city].to_numpy()[0][2:-1]\n",
    "    \n",
    "    # Correct family size by family_number\n",
    "    people_higher_than_6 = family_number[i] - np.sum(family_size_array[0:-1]*np.arange(1, 6))\n",
    "    higher_than_6_array = np.array([family_size_array[-1]])\n",
    "    \n",
    "    b = np.array([people_higher_than_6, family_size_array[-1]])\n",
    "    A = np.array([[j, 1] for j in range(6, 13)])  # Changed to include sizes 6 to 12\n",
    "    \n",
    "    res = lsq_linear(A.T, b, bounds=(0, 10**10), lsmr_tol='auto', verbose=0)\n",
    "    family_size_array_tmp = np.append(family_size_array[0:-1], np.array(res.x))\n",
    "    \n",
    "    family_size_array = family_size_array_tmp\n",
    "    family_size_dict[city.replace(' ', '')] = np.array(family_size_array/sum(family_size_array), dtype=float)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.bar(np.arange(len(family_size_array)), family_size_array)\n",
    "    plt.xticks(np.arange(len(family_size_array)), np.arange(len(family_size_array))+1)\n",
    "    plt.xlabel('Household family size')\n",
    "    plt.ylabel('Amount of household')\n",
    "    plt.close()  # Comment out this line if you want to see the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5b1a70",
   "metadata": {},
   "source": [
    "## 1.2 School"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0a86265",
   "metadata": {},
   "outputs": [],
   "source": [
    "school_p = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf8e24b",
   "metadata": {},
   "source": [
    "### 1.2.1 Elementary school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43077b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "elementary_school_data = pd.read_excel(demographic_data_path/'國民小學校別資料.xls')\n",
    "elementary_school_data.columns = elementary_school_data.iloc[1]\n",
    "elementary_school_data = elementary_school_data[2::]\n",
    "elementary_school_data = elementary_school_data[elementary_school_data['縣市名稱'] != '金門縣']\n",
    "elementary_school_data = elementary_school_data[elementary_school_data['縣市名稱'] != '連江縣']\n",
    "elementary_school_data = elementary_school_data[elementary_school_data['縣市名稱'] != '澎湖縣']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01b1423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_index = elementary_school_data.columns[6:11+1]\n",
    "class_index_english = ['1st-grade', '2nd-grade',\n",
    "                       '3rd-grade', '4th-grade', '5th-grade', '6th-grade']\n",
    "student_number_index = elementary_school_data.columns[15:26+1]\n",
    "for i in range(len(class_index)):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    # Remove empty schools\n",
    "    elementary_school_data_temp = elementary_school_data[elementary_school_data[class_index[i]] != 0]\n",
    "    student_per_class = (elementary_school_data_temp[student_number_index[2*i]] +\n",
    "                         elementary_school_data_temp[student_number_index[2*i+1]])/elementary_school_data_temp[class_index[i]]\n",
    "    n = plt.hist(student_per_class, bins=range(int(max(student_per_class))))\n",
    "    probability = np.zeros(int(n[1].max()) + 1)\n",
    "    probability[n[1][0:-1].astype(int)] = n[0]/sum(n[0])\n",
    "    school_p[i+7] = probability\n",
    "\n",
    "    plt.xticks(np.arange(0, int(max(student_per_class)), 5))\n",
    "    plt.xlabel('Number of ' + class_index_english[i] + ' students per class')\n",
    "    plt.ylabel('Number of school')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0547ce",
   "metadata": {},
   "source": [
    "### 1.2.2 Junior high school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "748d8e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "junior_high_school_data = pd.read_excel(demographic_data_path/'國民中學校別資料.xlsx')\n",
    "junior_high_school_data.columns = junior_high_school_data.iloc[1]\n",
    "junior_high_school_data = junior_high_school_data[2::]\n",
    "junior_high_school_data = junior_high_school_data[junior_high_school_data['縣市名稱'] != '金門縣']\n",
    "junior_high_school_data = junior_high_school_data[junior_high_school_data['縣市名稱'] != '連江縣']\n",
    "junior_high_school_data = junior_high_school_data[junior_high_school_data['縣市名稱'] != '澎湖縣']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2521d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_index = junior_high_school_data.columns[6:8+1]\n",
    "class_index_english = ['7th-grade', '8th-grade', '9th-grade']\n",
    "student_number_index = junior_high_school_data.columns[12:17+1]\n",
    "for i in range(len(class_index)):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    # Remove empty schools\n",
    "    junior_high_school_data_temp = junior_high_school_data[\n",
    "        junior_high_school_data[class_index[i]] != 0]\n",
    "    student_per_class = (junior_high_school_data_temp[student_number_index[2*i]] +\n",
    "                         junior_high_school_data_temp[student_number_index[2*i+1]])/junior_high_school_data_temp[class_index[i]]\n",
    "    n = plt.hist(student_per_class, bins=range(int(max(student_per_class))))\n",
    "    probability = np.zeros(int(n[1].max())+1)\n",
    "    probability[n[1][0:-1].astype(int)] = n[0]/sum(n[0])\n",
    "    school_p[i+13] = probability\n",
    "\n",
    "    plt.xticks(np.arange(0, int(max(student_per_class)), 5))\n",
    "    plt.xlabel('Amount of ' + class_index_english[i] + ' student per class')\n",
    "    plt.ylabel('Amount of school')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc135d52",
   "metadata": {},
   "source": [
    "### 1.2.3 Senior high school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72bb8775",
   "metadata": {},
   "outputs": [],
   "source": [
    "senior_high_school_data = pd.read_excel(demographic_data_path/'高級中等學校校別資料檔.xls')\n",
    "senior_high_school_data.columns = senior_high_school_data.iloc[1]\n",
    "senior_high_school_data = senior_high_school_data[2::]\n",
    "senior_high_school_data = senior_high_school_data[senior_high_school_data['縣市名稱'] != '金門縣']\n",
    "senior_high_school_data = senior_high_school_data[senior_high_school_data['縣市名稱'] != '連江縣']\n",
    "senior_high_school_data = senior_high_school_data[senior_high_school_data['縣市名稱'] != '澎湖縣']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "512edb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 附設國中部\n",
    "senior_high_school_data = senior_high_school_data[~(\n",
    "    senior_high_school_data['學程(等級)別'] == 'J')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38057903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_index = senior_high_school_data.columns[9:11+1]\n",
    "class_index_english = ['1st-grade', '2nd-grade', '3rd-grade']\n",
    "student_number_index = senior_high_school_data.columns[16:21+1]\n",
    "for i in range(len(class_index)):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    # Remove empty schools\n",
    "    senior_high_school_data_temp = senior_high_school_data[\n",
    "        senior_high_school_data[class_index[i]] != 0]\n",
    "    student_per_class = (senior_high_school_data_temp[student_number_index[2*i]] +\n",
    "                         senior_high_school_data_temp[student_number_index[2*i+1]])/senior_high_school_data_temp[class_index[i]]\n",
    "    n = plt.hist(student_per_class, bins=range(int(max(student_per_class))))\n",
    "    probability = np.zeros(int(n[1].max())+1)\n",
    "    probability[n[1][0:-1].astype(int)] = n[0]/sum(n[0])\n",
    "    school_p[i+16] = probability\n",
    "\n",
    "    plt.xticks(np.arange(0, int(max(student_per_class)), 5))\n",
    "    plt.xlabel('Amount of ' + class_index_english[i] + ' student per class')\n",
    "    plt.ylabel('Amount of school')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b87793c",
   "metadata": {},
   "source": [
    "### 1.2.4 University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25b8f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "university_data = pd.read_excel(demographic_data_path/'大專校院各校科系別學生數.xlsx')\n",
    "university_data.columns = university_data.iloc[1]\n",
    "university_data = university_data[2::]\n",
    "# Only extract bachelor students\n",
    "university_data = university_data[university_data['等級別'] == 'B 學士']\n",
    "university_data = university_data[university_data['縣市名稱'] != '71 金門縣']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc64c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_index = university_data.columns[9:22+1]\n",
    "class_index_english = ['1st-grade', '2nd-grade', '3rd-grade', '4th-grade']\n",
    "student_number_index = university_data.columns[9:16+1]\n",
    "for i in range(len(class_index_english)):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    student_number = university_data[student_number_index[2*i]\n",
    "                                     ] + university_data[student_number_index[2*i+1]]\n",
    "    student_number = student_number[student_number > 0]  # Remove empty grade\n",
    "    n = plt.hist(student_number, bins=range(int(max(student_number))))\n",
    "    probability = np.zeros(int(n[1].max())+1)\n",
    "    probability[n[1][0:-1].astype(int)] = n[0]/sum(n[0])\n",
    "    school_p[i+19] = probability\n",
    "\n",
    "    plt.xticks(np.arange(0, int(max(student_number)), 20))\n",
    "    plt.xlabel('Amount of ' + class_index_english[i] + ' students')\n",
    "    plt.ylabel('Amount of department')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f6d3c",
   "metadata": {},
   "source": [
    "## 1.3 Workplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf866c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "workplace_p = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f0f7cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "workplace_data = pd.read_excel(demographic_data_path/'工業及服務業企業單位經營概況.xls', sheet_name='18')\n",
    "workplace_data = workplace_data.iloc[15:-1, 1:17+1]\n",
    "workplace_data = workplace_data.drop(['Unnamed: 11'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7905931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['<5', '5~9', '10~19', '20~29', '30~39', '40~49',\n",
    "           '50~99', '100~199', '200~299', '300~499', '500~999', '>1000']\n",
    "for row in workplace_data.iloc:\n",
    "    if type(row.iloc[0]) == str:\n",
    "        plt.figure()\n",
    "        number_of_companys = row.iloc[3:14+1]\n",
    "        plt.bar(np.arange(len(headers)), number_of_companys)\n",
    "        plt.xticks(np.arange(len(headers)), headers, rotation=45)\n",
    "        # print(row.iloc[0])\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "        probability = number_of_companys.to_numpy()/sum(number_of_companys.to_numpy())\n",
    "        workplace_p_tmp = np.zeros(1000)\n",
    "        # 1~999\n",
    "        workplace_p_tmp[1:5] = probability[0]/4\n",
    "        workplace_p_tmp[5:10] = probability[1]/5\n",
    "        workplace_p_tmp[10:20] = probability[2]/10\n",
    "        workplace_p_tmp[20:30] = probability[3]/10\n",
    "        workplace_p_tmp[30:40] = probability[4]/10\n",
    "        workplace_p_tmp[40:50] = probability[5]/10\n",
    "        workplace_p_tmp[50:100] = probability[6]/50\n",
    "        workplace_p_tmp[100:200] = probability[7]/100\n",
    "        workplace_p_tmp[200:300] = probability[8]/100\n",
    "        workplace_p_tmp[300:500] = probability[9]/200\n",
    "        workplace_p_tmp[500:1000] = probability[10]/500\n",
    "        # >1000\n",
    "        if probability[11] != 0:\n",
    "            append_numbers = np.ceil(probability[11]/workplace_p_tmp[-1])\n",
    "            workplace_p_tmp = np.append(workplace_p_tmp, np.ones(\n",
    "                int(append_numbers))*(probability[11]/append_numbers))\n",
    "        workplace_p_tmp = np.array(workplace_p_tmp, dtype=float)\n",
    "\n",
    "        workplace_p[row.iloc[0]] = workplace_p_tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6a7ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "workplace_data = pd.read_excel(demographic_data_path/'工業及服務業企業單位經營概況.xls', sheet_name='18-2')\n",
    "workplace_data = workplace_data.iloc[14:-1, 1:17+1]\n",
    "workplace_data = workplace_data.drop(['Unnamed: 11'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['<5', '5~9', '10~19', '20~29', '30~39', '40~49',\n",
    "           '50~99', '100~199', '200~299', '300~499', '500~999', '>1000']\n",
    "for row in workplace_data.iloc:\n",
    "    if type(row.iloc[0]) == str:\n",
    "        plt.figure()\n",
    "        number_of_companys = row.iloc[3:14+1]\n",
    "        plt.bar(np.arange(len(headers)), row.iloc[3:14+1])\n",
    "        plt.xticks(np.arange(len(headers)), headers, rotation=45)\n",
    "        print(row.iloc[0])\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "        probability = number_of_companys.to_numpy()/sum(number_of_companys.to_numpy())\n",
    "        workplace_p_tmp = np.zeros(1000)\n",
    "        # 1~999\n",
    "        workplace_p_tmp[1:5] = probability[0]/4\n",
    "        workplace_p_tmp[5:10] = probability[1]/5\n",
    "        workplace_p_tmp[10:20] = probability[2]/10\n",
    "        workplace_p_tmp[20:30] = probability[3]/10\n",
    "        workplace_p_tmp[30:40] = probability[4]/10\n",
    "        workplace_p_tmp[40:50] = probability[5]/10\n",
    "        workplace_p_tmp[50:100] = probability[6]/50\n",
    "        workplace_p_tmp[100:200] = probability[7]/100\n",
    "        workplace_p_tmp[200:300] = probability[8]/100\n",
    "        workplace_p_tmp[300:500] = probability[9]/200\n",
    "        workplace_p_tmp[500:1000] = probability[10]/500\n",
    "        # >1000\n",
    "        if probability[11] != 0:\n",
    "            append_numbers = np.ceil(probability[11]/workplace_p_tmp[-1])\n",
    "            workplace_p_tmp = np.append(workplace_p_tmp, np.ones(\n",
    "                int(append_numbers))*(probability[11]/append_numbers))\n",
    "        workplace_p_tmp = np.array(workplace_p_tmp, dtype=float)\n",
    "\n",
    "        workplace_p[row.iloc[0]] = workplace_p_tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffdbbaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "workplace_data = pd.read_excel(demographic_data_path/'工業及服務業企業單位經營概況.xls', sheet_name='18-3')\n",
    "workplace_data = workplace_data.iloc[14:-1, 1:17+1]\n",
    "workplace_data = workplace_data.drop(['Unnamed: 11'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4454d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['<5', '5~9', '10~19', '20~29', '30~39', '40~49',\n",
    "           '50~99', '100~199', '200~299', '300~499', '500~999', '>1000']\n",
    "for row in workplace_data.iloc:\n",
    "    if type(row.iloc[0]) == str:\n",
    "        plt.figure()\n",
    "        number_of_companys = row.iloc[3:14+1]\n",
    "        plt.bar(np.arange(len(headers)), row.iloc[3:14+1])\n",
    "        plt.xticks(np.arange(len(headers)), headers, rotation=45)\n",
    "        print(row.iloc[0])\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "        probability = number_of_companys.to_numpy()/sum(number_of_companys.to_numpy())\n",
    "        workplace_p_tmp = np.zeros(1000)\n",
    "        # 1~999\n",
    "        workplace_p_tmp[1:5] = probability[0]/4\n",
    "        workplace_p_tmp[5:10] = probability[1]/5\n",
    "        workplace_p_tmp[10:20] = probability[2]/10\n",
    "        workplace_p_tmp[20:30] = probability[3]/10\n",
    "        workplace_p_tmp[30:40] = probability[4]/10\n",
    "        workplace_p_tmp[40:50] = probability[5]/10\n",
    "        workplace_p_tmp[50:100] = probability[6]/50\n",
    "        workplace_p_tmp[100:200] = probability[7]/100\n",
    "        workplace_p_tmp[200:300] = probability[8]/100\n",
    "        workplace_p_tmp[300:500] = probability[9]/200\n",
    "        workplace_p_tmp[500:1000] = probability[10]/500\n",
    "        # >1000\n",
    "        if probability[11] != 0:\n",
    "            append_numbers = np.ceil(probability[11]/workplace_p_tmp[-1])\n",
    "            workplace_p_tmp = np.append(workplace_p_tmp, np.ones(\n",
    "                int(append_numbers))*(probability[11]/append_numbers))\n",
    "        workplace_p_tmp = np.array(workplace_p_tmp, dtype=float)\n",
    "\n",
    "        workplace_p[row.iloc[0]] = workplace_p_tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63e741a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "workplace_data = pd.read_excel(demographic_data_path/'工業及服務業企業單位經營概況.xls', sheet_name='18-4')\n",
    "workplace_data = workplace_data.iloc[14:-1, 1:17+1]\n",
    "workplace_data = workplace_data.drop(['Unnamed: 11'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db647ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['<5', '5~9', '10~19', '20~29', '30~39', '40~49',\n",
    "           '50~99', '100~199', '200~299', '300~499', '500~999', '>1000']\n",
    "for row in workplace_data.iloc:\n",
    "    if type(row.iloc[0]) == str:\n",
    "        plt.figure()\n",
    "        number_of_companys = row.iloc[3:14+1]\n",
    "        plt.bar(np.arange(len(headers)), row.iloc[3:14+1])\n",
    "        plt.xticks(np.arange(len(headers)), headers, rotation=45)\n",
    "        print(row.iloc[0])\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "        probability = number_of_companys.to_numpy()/sum(number_of_companys.to_numpy())\n",
    "        workplace_p_tmp = np.zeros(1000)\n",
    "        # 1~999\n",
    "        workplace_p_tmp[1:5] = probability[0]/4\n",
    "        workplace_p_tmp[5:10] = probability[1]/5\n",
    "        workplace_p_tmp[10:20] = probability[2]/10\n",
    "        workplace_p_tmp[20:30] = probability[3]/10\n",
    "        workplace_p_tmp[30:40] = probability[4]/10\n",
    "        workplace_p_tmp[40:50] = probability[5]/10\n",
    "        workplace_p_tmp[50:100] = probability[6]/50\n",
    "        workplace_p_tmp[100:200] = probability[7]/100\n",
    "        workplace_p_tmp[200:300] = probability[8]/100\n",
    "        workplace_p_tmp[300:500] = probability[9]/200\n",
    "        workplace_p_tmp[500:1000] = probability[10]/500\n",
    "        # >1000\n",
    "        if probability[11] != 0:\n",
    "            append_numbers = np.ceil(probability[11]/workplace_p_tmp[-1])\n",
    "            workplace_p_tmp = np.append(workplace_p_tmp, np.ones(\n",
    "                int(append_numbers))*(probability[11]/append_numbers))\n",
    "        workplace_p_tmp = np.array(workplace_p_tmp, dtype=float)\n",
    "\n",
    "        workplace_p[row.iloc[0]] = workplace_p_tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c1b73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "workplace_data = pd.read_excel(demographic_data_path/'工業及服務業企業單位經營概況.xls', sheet_name='18-5')\n",
    "workplace_data = workplace_data.iloc[14:-1, 1:17+1]\n",
    "workplace_data = workplace_data.drop(['Unnamed: 11'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa27d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['<5', '5~9', '10~19', '20~29', '30~39', '40~49',\n",
    "           '50~99', '100~199', '200~299', '300~499', '500~999', '>1000']\n",
    "for row in workplace_data.iloc:\n",
    "    if type(row.iloc[0]) == str:\n",
    "        plt.figure()\n",
    "        number_of_companys = row.iloc[3:14+1]\n",
    "        plt.bar(np.arange(len(headers)), row.iloc[3:14+1])\n",
    "        plt.xticks(np.arange(len(headers)), headers, rotation=45)\n",
    "        print(row.iloc[0])\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "        probability = number_of_companys.to_numpy()/sum(number_of_companys.to_numpy())\n",
    "        workplace_p_tmp = np.zeros(1000)\n",
    "        # 1~999\n",
    "        workplace_p_tmp[1:5] = probability[0]/4\n",
    "        workplace_p_tmp[5:10] = probability[1]/5\n",
    "        workplace_p_tmp[10:20] = probability[2]/10\n",
    "        workplace_p_tmp[20:30] = probability[3]/10\n",
    "        workplace_p_tmp[30:40] = probability[4]/10\n",
    "        workplace_p_tmp[40:50] = probability[5]/10\n",
    "        workplace_p_tmp[50:100] = probability[6]/50\n",
    "        workplace_p_tmp[100:200] = probability[7]/100\n",
    "        workplace_p_tmp[200:300] = probability[8]/100\n",
    "        workplace_p_tmp[300:500] = probability[9]/200\n",
    "        workplace_p_tmp[500:1000] = probability[10]/500\n",
    "        # >1000\n",
    "        if probability[11] != 0:\n",
    "            append_numbers = np.ceil(probability[11]/workplace_p_tmp[-1])\n",
    "            workplace_p_tmp = np.append(workplace_p_tmp, np.ones(\n",
    "                int(append_numbers))*(probability[11]/append_numbers))\n",
    "        workplace_p_tmp = np.array(workplace_p_tmp, dtype=float)\n",
    "\n",
    "        workplace_p[row.iloc[0]] = workplace_p_tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "619f7a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "workplace_data = pd.read_excel(demographic_data_path/'工業及服務業企業單位經營概況.xls', sheet_name='18-6')\n",
    "workplace_data = workplace_data.iloc[14:-1, 1:17+1]\n",
    "workplace_data = workplace_data.drop(['Unnamed: 11'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1938faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['<5', '5~9', '10~19', '20~29', '30~39', '40~49',\n",
    "           '50~99', '100~199', '200~299', '300~499', '500~999', '>1000']\n",
    "for row in workplace_data.iloc:\n",
    "    if type(row.iloc[0]) == str:\n",
    "        plt.figure()\n",
    "        number_of_companys = row.iloc[3:14+1]\n",
    "        plt.bar(np.arange(len(headers)), row.iloc[3:14+1])\n",
    "        plt.xticks(np.arange(len(headers)), headers, rotation=45)\n",
    "        # plt.show()\n",
    "        print(row.iloc[0])\n",
    "        plt.close()\n",
    "        probability = number_of_companys.to_numpy()/sum(number_of_companys.to_numpy())\n",
    "        workplace_p_tmp = np.zeros(1000)\n",
    "        # 1~999\n",
    "        workplace_p_tmp[1:5] = probability[0]/4\n",
    "        workplace_p_tmp[5:10] = probability[1]/5\n",
    "        workplace_p_tmp[10:20] = probability[2]/10\n",
    "        workplace_p_tmp[20:30] = probability[3]/10\n",
    "        workplace_p_tmp[30:40] = probability[4]/10\n",
    "        workplace_p_tmp[40:50] = probability[5]/10\n",
    "        workplace_p_tmp[50:100] = probability[6]/50\n",
    "        workplace_p_tmp[100:200] = probability[7]/100\n",
    "        workplace_p_tmp[200:300] = probability[8]/100\n",
    "        workplace_p_tmp[300:500] = probability[9]/200\n",
    "        workplace_p_tmp[500:1000] = probability[10]/500\n",
    "        # >1000\n",
    "        if probability[11] != 0:\n",
    "            append_numbers = np.ceil(probability[11]/workplace_p_tmp[-1])\n",
    "            workplace_p_tmp = np.append(workplace_p_tmp, np.ones(\n",
    "                int(append_numbers))*(probability[11]/append_numbers))\n",
    "        workplace_p_tmp = np.array(workplace_p_tmp, dtype=float)\n",
    "\n",
    "        workplace_p[row.iloc[0]] = workplace_p_tmp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31825a82",
   "metadata": {},
   "source": [
    "## 1.4 Health care"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e87b712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_care_data = pd.read_excel(demographic_data_path/'醫院平均每日醫療服務量統計.xls')\n",
    "health_care_data.columns = health_care_data.iloc[3]\n",
    "health_care_data = health_care_data[9::]\n",
    "health_care_data = health_care_data.drop(['合計'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "594d8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean colunms and rows\n",
    "health_care_data = health_care_data.drop(16)  # Drop 非公立醫院\n",
    "health_care_data = health_care_data.drop(23)  # Drop 私立牙醫醫院\n",
    "hospital_lists = health_care_data.iloc[:, 0].to_list()  # Save hospital names\n",
    "hospital_numbers = health_care_data.iloc[:, 1].to_numpy()\n",
    "# Clean data\n",
    "for i in range(len(hospital_lists)):\n",
    "    hospital_lists[i] = hospital_lists[i][2::]\n",
    "health_care_data = health_care_data.iloc[:, 2:-1]\n",
    "\n",
    "# Hospital name probability\n",
    "hospital_p = health_care_data.sum(axis=1).to_numpy()\n",
    "hospital_p = np.array(hospital_p/sum(hospital_p), dtype=float)\n",
    "hospital_p = [hospital_lists, hospital_p]\n",
    "\n",
    "hospital_size_p = {}\n",
    "hospital_sizes = {}\n",
    "for i, hospital_list in enumerate(hospital_lists):\n",
    "    if sum(health_care_data.iloc[i].to_numpy()) > 0:\n",
    "        hospital_size_p[hospital_list] = np.array(health_care_data.iloc[i].to_numpy(\n",
    "        )/sum(health_care_data.iloc[i].to_numpy()), dtype=float)\n",
    "    else:\n",
    "        hospital_size_p[hospital_list] = 0\n",
    "    hospital_sizes[hospital_list] = np.array(\n",
    "        health_care_data.iloc[i].to_numpy()/hospital_numbers[i], dtype=float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7097d7b",
   "metadata": {},
   "source": [
    "## 1.4 Municipality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5e67bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "municipality_data = pd.read_excel(demographic_data_path/'municipality_data.xls', sheet_name='02-縣市別')\n",
    "municipality_data.columns = municipality_data.iloc[1]\n",
    "municipality_data = municipality_data[4:24+1]\n",
    "municipality_data = municipality_data[municipality_data['區  域  別'] != '臺灣省']\n",
    "municipality_data = municipality_data[municipality_data['區  域  別'] != '福建省']\n",
    "municipality_data = municipality_data[municipality_data['區  域  別'] != '澎湖縣']\n",
    "municipality_data = municipality_data[municipality_data['區  域  別'] != '金門縣']\n",
    "municipality_data = municipality_data[municipality_data['區  域  別'] != '連江縣']\n",
    "municipality_data = municipality_data[municipality_data['區  域  別'] != '東沙群島']\n",
    "municipality_data = municipality_data[municipality_data['區  域  別'] != '南沙群島']\n",
    "\n",
    "municipality_data = dict(\n",
    "    zip(municipality_data['區  域  別'], municipality_data['人　　口　　數']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b521d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "municipality_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a547c47b",
   "metadata": {},
   "source": [
    "## 1.5 Age and gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1dd0c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_gender_data = pd.read_csv(demographic_data_path/'Single_Age_Population_2022-04-25.csv')\n",
    "age_male_data = age_gender_data[age_gender_data['項目']\n",
    "                                == '男性']['2022'].to_numpy()\n",
    "age_female_data = age_gender_data[age_gender_data['項目']\n",
    "                                  == '女性']['2022'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3572c481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population pyramid\n",
    "import plotly.graph_objects as gp\n",
    "\n",
    "ages = np.arange(100+1)\n",
    "\n",
    "# Creating instance of the figure\n",
    "fig = gp.Figure()\n",
    "\n",
    "# Adding Male data to the figure\n",
    "fig.add_trace(gp.Bar(y=ages, x=age_male_data,\n",
    "              name='Male', orientation='h'))\n",
    "\n",
    "# Adding Female data to the figure\n",
    "fig.add_trace(gp.Bar(y=ages, x=-age_female_data,\n",
    "              name='Female', orientation='h'))\n",
    "\n",
    "# Updating the layout for our graph\n",
    "fig.update_layout(title='Population Pyramid of Taiwan 2022',\n",
    "                  title_font_size=22, barmode='relative',\n",
    "                  bargap=0.0, bargroupgap=0,\n",
    "                  xaxis=dict(tickvals=[-100000, -200000, 0, 100000, 200000], ticktext=[\n",
    "                             '100k', '200k', '0', '100k', '200k'], title='Population', title_font_size=14)\n",
    "                  )\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc9e0bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save age\n",
    "age_p = np.array(age_male_data) + np.array(age_female_data)\n",
    "age_p = age_p/np.sum(age_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40825479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save gender\n",
    "gender_age_groups = np.vstack([age_male_data, age_female_data]).T\n",
    "gender_p = np.empty([0, 2])\n",
    "for i in gender_age_groups:\n",
    "    gender_p = np.vstack([gender_p, i/sum(i)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dbee8d",
   "metadata": {},
   "source": [
    "## 1.6 Student rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "301cc88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide the probability of the person is a student or not.\n",
    "male_student_p = np.zeros(101)\n",
    "female_student_p = np.zeros(101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e762fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age 7 to 12 (elementary school)\n",
    "male_elementary_student_p = elementary_school_data[[\n",
    "    '1年級男', '2年級男', '3年級男', '4年級男', '5年級男', '6年級男']].sum().to_numpy()/age_male_data[7:12+1]\n",
    "female_elementary_student_p = elementary_school_data[[\n",
    "    '1年級女', '2年級女', '3年級女', '4年級女', '5年級女', '6年級女']].sum().to_numpy()/age_female_data[7:12+1]\n",
    "\n",
    "\n",
    "male_student_p[7:12+1] = male_elementary_student_p\n",
    "female_student_p[7:12+1] = female_elementary_student_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa5c0e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age 13 to 15 (junior high school)\n",
    "male_junior_high_school_student_p = junior_high_school_data[[\n",
    "    '7年級男', '8年級男', '9年級男']].sum().to_numpy()/age_male_data[13:15+1]\n",
    "female_junior_high_school_student_p = junior_high_school_data[[\n",
    "    '7年級女', '8年級女', '9年級女']].sum().to_numpy()/age_female_data[13:15+1]\n",
    "\n",
    "male_student_p[13:15+1] = male_junior_high_school_student_p\n",
    "female_student_p[13:15+1] = female_junior_high_school_student_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4996d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age 16 to 18 (senior high school)\n",
    "male_senior_high_school_p = senior_high_school_data[[\n",
    "    '一年級男', '二年級男', '三年級男']].sum().to_numpy()/age_male_data[16:18+1]\n",
    "female_senior_high_school_p = senior_high_school_data[[\n",
    "    '一年級女', '二年級女', '三年級女']].sum().to_numpy()/age_female_data[16:18+1]\n",
    "\n",
    "male_student_p[16:18+1] = male_senior_high_school_p\n",
    "female_student_p[16:18+1] = female_senior_high_school_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "08889741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age 19 to 22 (university)\n",
    "male_university_p = university_data[[\n",
    "    '一年級男生', '二年級男生', '三年級男生', '四年級男生']].sum().to_numpy()/age_male_data[19:22+1]\n",
    "female_university_p = university_data[[\n",
    "    '一年級女生', '二年級女生', '三年級女生', '四年級女生']].sum().to_numpy()/age_female_data[19:22+1]\n",
    "\n",
    "male_student_p[19:22+1] = male_university_p\n",
    "female_student_p[19:22+1] = female_university_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c3165d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace >1 to 1\n",
    "male_student_p[male_student_p > 1] = 1\n",
    "female_student_p[female_student_p > 1] = 1\n",
    "\n",
    "student_p = np.vstack([male_student_p, female_student_p])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baa7b7f",
   "metadata": {},
   "source": [
    "## 1.7 Employment rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "acf98f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emplolyment rate in each age group\n",
    "employment_data = pd.read_excel(demographic_data_path/'就業率-年齡別-20220428.xlsx')\n",
    "employment_data.columns = employment_data.iloc[2]\n",
    "employment_data = employment_data.iloc[3::]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ee8a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_p = employment_data.to_numpy()\n",
    "employment_p = employment_p[0][1::]\n",
    "\n",
    "male_employment_p = employment_p[0:9+1]/100\n",
    "female_employment_p = employment_p[10::]/100\n",
    "\n",
    "# Repeat emelent in the same age group\n",
    "male_employment_p = np.repeat(male_employment_p, 5)  # Split age group\n",
    "female_employment_p = np.repeat(female_employment_p, 5)  # Split age group\n",
    "\n",
    "# Zero padding\n",
    "male_employment_p = np.insert(\n",
    "    male_employment_p, obj=0, values=np.zeros(15))  # Zero pading for age under 15\n",
    "male_employment_p = np.append(male_employment_p, np.zeros(\n",
    "    36))  # Append last value for age above 65\n",
    "female_employment_p = np.insert(\n",
    "    female_employment_p, obj=0, values=np.zeros(15))  # Zero pading for age under 15\n",
    "female_employment_p = np.append(female_employment_p, np.zeros(\n",
    "    36))  # Append last value for age above 65\n",
    "\n",
    "employment_p = np.vstack([male_employment_p, female_employment_p])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a814d17c",
   "metadata": {},
   "source": [
    "## 1.8 Part-time and full-tim job probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ab0f361",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_type_data = pd.read_excel(demographic_data_path/'mtable19.xlsx')\n",
    "job_type_data = job_type_data.iloc[9:29]\n",
    "\n",
    "# Drop 工業 and 服務業\n",
    "job_type_data = job_type_data[~(\n",
    "    job_type_data['Unnamed: 1'] == '工業\\n  Goods-Producing Industries')]\n",
    "job_type_data = job_type_data[~(\n",
    "    job_type_data['Unnamed: 1'] == '服務業\\n  Services-Producing Industries')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "17a3b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_job_type_data = job_type_data.iloc[:, 13:14+1]\n",
    "female_job_type_data = job_type_data.iloc[:, 23:24+1]\n",
    "\n",
    "job_list = job_type_data['Unnamed: 1'].to_numpy()\n",
    "\n",
    "male_part_time_job_p = male_job_type_data['Unnamed: 14'].replace(\n",
    "    '-', 0).to_numpy(dtype=float)/sum(male_job_type_data['Unnamed: 14'].replace('-', 0).to_numpy(dtype=float))\n",
    "male_full_time_job_p = male_job_type_data['Unnamed: 13'].replace(\n",
    "    '-', 0).to_numpy(dtype=float)/sum(male_job_type_data['Unnamed: 13'].replace('-', 0).to_numpy(dtype=float))\n",
    "\n",
    "female_part_time_job_p = female_job_type_data['Unnamed: 24'].replace(\n",
    "    '-', 0).to_numpy(dtype=float)/sum(female_job_type_data['Unnamed: 24'].replace('-', 0).to_numpy(dtype=float))\n",
    "female_full_time_job_p = female_job_type_data['Unnamed: 23'].replace(\n",
    "    '-', 0).to_numpy(dtype=float)/sum(female_job_type_data['Unnamed: 23'].replace('-', 0).to_numpy(dtype=float))\n",
    "\n",
    "part_time_job_p = np.vstack([male_part_time_job_p, female_part_time_job_p])\n",
    "full_time_job_p = np.vstack([male_full_time_job_p, female_full_time_job_p])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6955ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_p = {}\n",
    "\n",
    "# Clean job_list\n",
    "for i in range(len(job_list)):\n",
    "    job_list[i] = job_list[i].split('\\n')[0].replace(' ', '')\n",
    "job_list = job_list[job_list != '農、林、漁、牧業']\n",
    "job_list = job_list[job_list != '公共行政及國防；強制性社會安全']\n",
    "job_list[8] = '出版、影音製作、傳播及資通訊服務業'\n",
    "job_list[9] = '金融及保險業、強制性社會安全'\n",
    "job_list[13] = '教育業(註)'\n",
    "\n",
    "\n",
    "job_p['job_list'] = job_list\n",
    "\n",
    "part_time_job_p = np.delete(\n",
    "    part_time_job_p, (0, 14), axis=1)  # Remove non exist job\n",
    "row_sums = part_time_job_p.sum(axis=1)\n",
    "part_time_job_p = part_time_job_p/row_sums[:, np.newaxis]\n",
    "job_p['part_time_job_p'] = part_time_job_p\n",
    "\n",
    "full_time_job_p = np.delete(\n",
    "    full_time_job_p, (0, 14), axis=1)  # Remove non exist job\n",
    "row_sums = full_time_job_p.sum(axis=1)\n",
    "full_time_job_p = full_time_job_p/row_sums[:, np.newaxis]\n",
    "job_p['full_time_job_p'] = full_time_job_p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2bb6b0",
   "metadata": {},
   "source": [
    "## 1.9 Save demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "744f5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# population_size = 23215015\n",
    "population_size = 23008366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc541aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_parameters = [age_p, gender_p, student_p, employment_p, job_p, family_size_dict, municipality_data,\n",
    "                school_p, workplace_p, hospital_p, hospital_size_p, hospital_sizes, population_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a71cf6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_p_state:\n",
    "    with open('./variable/demographic_parameters.pkl', 'wb') as f:\n",
    "        pickle.dump(demographic_parameters, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6c1dd4",
   "metadata": {},
   "source": [
    "# 2. Course of disease parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e276c8e",
   "metadata": {},
   "source": [
    "## 2.1 Epidemic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbe105ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_parameters = np.array([])\n",
    "course_parameters_lb = np.array([])\n",
    "course_parameters_ub = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b7b724",
   "metadata": {},
   "source": [
    "### 2.1.1 Latent period\n",
    "\n",
    "Xin, Hualei, et al. \"Estimating the latent period of coronavirus disease 2019 (COVID-19).\" Clinical Infectious Diseases 74.9 (2022): 1678-1681."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2f75d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_period_shape = 4.05\n",
    "latent_period_shape_lb = 3.32\n",
    "latent_period_shape_ub = 5.13\n",
    "\n",
    "latent_period_scale = 1.35\n",
    "latent_period_scale_lb = 1.06\n",
    "latent_period_scale_ub = 1.67\n",
    "\n",
    "course_parameters = np.append(course_parameters, (latent_period_shape, latent_period_scale))\n",
    "course_parameters_lb = np.append(course_parameters_lb, (latent_period_shape_lb, latent_period_scale_lb))\n",
    "course_parameters_ub = np.append(course_parameters_ub, (latent_period_shape_ub, latent_period_scale_ub))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8fcdb8",
   "metadata": {},
   "source": [
    "### 2.1.2 Infectious period\n",
    "\n",
    "Sanche, Steven, et al. \"High contagiousness and rapid spread of severe acute respiratory syndrome coronavirus 2.\" Emerging infectious diseases 26.7 (2020): 1470."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3712b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "infectious_period_shape = 4\n",
    "infectious_period_shape_lb = 2\n",
    "infectious_period_shape_ub = 6\n",
    "\n",
    "infectious_period_mean = 10\n",
    "infectious_period_mean_lb = 4\n",
    "infectious_period_mean_ub = 14\n",
    "\n",
    "infectious_period_scale = infectious_period_mean/infectious_period_shape\n",
    "infectious_period_scale_lb = infectious_period_mean_lb/infectious_period_shape\n",
    "infectious_period_scale_ub = infectious_period_mean_ub/infectious_period_shape\n",
    "\n",
    "\n",
    "course_parameters = np.append(course_parameters, (infectious_period_shape, infectious_period_scale))\n",
    "course_parameters_lb = np.append(course_parameters_lb, (infectious_period_shape_lb, infectious_period_scale_lb))\n",
    "course_parameters_ub = np.append(course_parameters_ub, (infectious_period_shape_ub, infectious_period_scale_ub))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bc6da0",
   "metadata": {},
   "source": [
    "### 2.1.3 Incubation period\n",
    "\n",
    "Cheng, Hao-Yuan, et al. \"Contact tracing assessment of COVID-19 transmission dynamics in Taiwan and risk at different exposure periods before and after symptom onset.\" JAMA internal medicine 180.9 (2020): 1156-1163."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f8d2b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "incubation_period_shape = 1.55\n",
    "incubation_period_shape_lb = 0.73\n",
    "incubation_period_shape_ub = 2.93\n",
    "\n",
    "incubation_period_scale = 3.32\n",
    "incubation_period_scale_lb = 1.6\n",
    "incubation_period_scale_ub = 8.79\n",
    "\n",
    "course_parameters = np.append(course_parameters, (incubation_period_shape, incubation_period_scale))\n",
    "course_parameters_lb = np.append(course_parameters_lb, (incubation_period_shape_lb, incubation_period_scale_lb))\n",
    "course_parameters_ub = np.append(course_parameters_ub, (incubation_period_shape_ub, incubation_period_scale_ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce125b",
   "metadata": {},
   "source": [
    "### 2.1.4 Fit Gamma distribution to days symptomatic to confirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "328042ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = Path('./data/structured_course_of_disease_data')\n",
    "Taiwan_data_sheet = pd.read_excel(data_path/'figshare_taiwan_covid.xlsx', \n",
    "                                  sheet_name=0)\n",
    "Taiwan_data_sheet.columns = Taiwan_data_sheet.columns.str.strip().str.lower().str.\\\n",
    "    replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "\n",
    "# Clean data\n",
    "data_1_to_579 = clean_taiwan_data(Taiwan_data_sheet, 1, 579)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e3ae977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract course of disease data\n",
    "asymptomatic_to_symptom_days = extract_state_data(\n",
    "    data_1_to_579, 'earliest_infection_date', 'onset_of_symptom')\n",
    "asymptomatic_to_recover_days = extract_state_data(data_1_to_579, 'earliest_infection_date', 'recovery',\n",
    "                                                  'onset_of_symptom')\n",
    "\n",
    "symptom_to_icu_days = extract_state_data(\n",
    "    data_1_to_579, 'onset_of_symptom', 'icu')\n",
    "symptom_to_recover_days = extract_state_data(\n",
    "    data_1_to_579, 'onset_of_symptom', 'recovery', 'icu')\n",
    "\n",
    "icu_to_recover_days = extract_state_data(\n",
    "    data_1_to_579, 'icu', 'recovery')\n",
    "icu_to_dead_days = extract_state_data(\n",
    "    data_1_to_579, 'icu', 'death_date')                                                 \n",
    "\n",
    "asymptomatic_to_confirmed_days = extract_state_data(\n",
    "    data_1_to_579, 'earliest_infection_date', 'confirmed_date', 'onset_of_symptom')\n",
    "asymptomatic_to_confirmed_days = asymptomatic_to_confirmed_days.drop(asymptomatic_to_recover_days.index)\n",
    "symptomatic_to_confirmed_days = extract_state_data(\n",
    "    data_1_to_579, 'onset_of_symptom', 'confirmed_date')\n",
    "symptomatic_to_confirmed_days = symptomatic_to_confirmed_days.drop(symptom_to_icu_days.index)\n",
    "symptomatic_to_confirmed_days = symptomatic_to_confirmed_days.drop(symptom_to_recover_days.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7a27f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_fit_bootstrap(days_data, CI=0.68, allow_negative=False):\n",
    "    n_bootstrap = 1000\n",
    "    bootstrap_alphas = np.zeros(n_bootstrap)\n",
    "    bootstrap_locs = np.zeros(n_bootstrap)\n",
    "    bootstrap_scales = np.zeros(n_bootstrap)\n",
    "    for i in tqdm(range(n_bootstrap)):\n",
    "        bootstrap_sample = np.random.choice(days_data, size=len(days_data), replace=True)\n",
    "        alpha, loc, scale = stats.gamma.fit(bootstrap_sample)\n",
    "        if loc < 0:\n",
    "            if allow_negative:\n",
    "                pass\n",
    "            else:\n",
    "                \n",
    "                alpha, loc, scale = stats.gamma.fit(bootstrap_sample, floc=0)\n",
    "        bootstrap_alphas[i] = alpha\n",
    "        bootstrap_locs[i] = loc\n",
    "        bootstrap_scales[i] = scale\n",
    "\n",
    "    alpha = np.median(bootstrap_alphas)\n",
    "    alpha_lb = np.percentile(bootstrap_alphas, (1 - CI) / 2 * 100)\n",
    "    alpha_ub = np.percentile(bootstrap_alphas, (1 + CI) / 2 * 100)\n",
    "    loc = np.median(bootstrap_locs)\n",
    "    loc_lb = np.percentile(bootstrap_locs, (1 - CI) / 2 * 100)\n",
    "    loc_ub = np.percentile(bootstrap_locs, (1 + CI) / 2 * 100)\n",
    "    scale = np.median(bootstrap_scales)\n",
    "    scale_lb = np.percentile(bootstrap_scales, (1 - CI) / 2 * 100)\n",
    "    scale_ub = np.percentile(bootstrap_scales, (1 + CI) / 2 * 100)\n",
    "\n",
    "    return alpha, alpha_lb, alpha_ub, loc, loc_lb, loc_ub, scale, scale_lb, scale_ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dced97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(symptomatic_to_confirmed_days):\n",
    "    symptomatic_to_confirmed_days = symptomatic_to_confirmed_days.apply(lambda x: x.days).to_numpy()\n",
    "print('Number of cases:', len(symptomatic_to_confirmed_days))\n",
    "\n",
    "CI = 0.68\n",
    "alpha, alpha_lb, alpha_ub, loc, loc_lb, loc_ub, scale, scale_lb, scale_ub = gamma_fit_bootstrap(symptomatic_to_confirmed_days, CI)\n",
    "symptom_to_confirmed_shape = alpha\n",
    "symptom_to_confirmed_scale = scale\n",
    "symptom_to_confirmed_loc = loc\n",
    "print(f\"alpha: {alpha:.4f} ({alpha_lb:.4f}, {alpha_ub:.4f})\")\n",
    "print(f\"loc: {loc:.4f} ({loc_lb:.4f}, {loc_ub:.4f})\")\n",
    "print(f\"scale: {scale:.4f} ({scale_lb:.4f}, {scale_ub:.4f})\")\n",
    "\n",
    "# Plot the fitted distribution with confidence interval\n",
    "x = np.linspace(0, 80, 1000)\n",
    "y = stats.gamma.pdf(x, alpha, loc, scale)\n",
    "\n",
    "# Calculate confidence intervals\n",
    "y_lower = stats.gamma.pdf(x, alpha_lb, loc_lb, scale_lb)\n",
    "y_upper = stats.gamma.pdf(x, alpha_ub, loc_ub, scale_ub)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(symptomatic_to_confirmed_days, density=True, alpha=0.7, label='Taiwanese data')\n",
    "plt.plot(x, y, 'r-', lw=2, label='Fitted Gamma distribution')\n",
    "plt.fill_between(x, y_lower, y_upper, color='r', alpha=0.2, label=f'{CI*100}% CI')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Symptomatic to Confirmed Days')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "course_parameters = np.append(course_parameters, (alpha, scale, loc))\n",
    "course_parameters_lb = np.append(course_parameters_lb, (alpha_lb, scale_lb, loc_lb))\n",
    "course_parameters_ub = np.append(course_parameters_ub, (alpha_ub, scale_ub, loc_ub))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0058ba",
   "metadata": {},
   "source": [
    "### 2.1.5 Fit Gamma distribution to days asymptomatic to recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e89be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Since we only have 2 data points, there is no point to do bootstraping. We just use the Gamma fit\n",
    "\n",
    "if type(asymptomatic_to_recover_days) != np.ndarray:\n",
    "    asymptomatic_to_recover_days = asymptomatic_to_recover_days.apply(lambda x: x.days).to_numpy()\n",
    "print('Number of cases:', len(asymptomatic_to_recover_days))\n",
    "\n",
    "alpha, loc, scale = stats.gamma.fit(asymptomatic_to_recover_days, floc=0)\n",
    "# Set a 30% shift for alpha, loc, and scale\n",
    "alpha_lb = alpha*0.8\n",
    "alpha_ub = alpha*1.2\n",
    "loc_lb = loc\n",
    "loc_ub = loc\n",
    "scale_lb = scale*0.8\n",
    "scale_ub = scale*1.2\n",
    "# alpha, alpha_lb, alpha_ub, loc, loc_lb, loc_ub, scale, scale_lb, scale_ub = gamma_fit_bootstrap(asymptomatic_to_recover_days, CI)\n",
    "asymptomatic_to_recovered_shape = alpha\n",
    "asymptomatic_to_recovered_scale = scale\n",
    "asymptomatic_to_recovered_loc = loc\n",
    "\n",
    "print(f\"alpha: {alpha:.4f} ({alpha_lb:.4f}, {alpha_ub:.4f})\")\n",
    "print(f\"loc: {loc:.4f} ({loc_lb:.4f}, {loc_ub:.4f})\")\n",
    "print(f\"scale: {scale:.4f} ({scale_lb:.4f}, {scale_ub:.4f})\")\n",
    "\n",
    "# Plot the fitted distribution with confidence interval\n",
    "x = np.linspace(0, 80, 1000)\n",
    "y = stats.gamma.pdf(x, alpha, loc, scale)\n",
    "\n",
    "# Calculate confidence intervals\n",
    "y_lower = stats.gamma.pdf(x, alpha_lb, loc_lb, scale_lb)\n",
    "y_upper = stats.gamma.pdf(x, alpha_ub, loc_ub, scale_ub)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(asymptomatic_to_recover_days, density=True, alpha=0.7, label='Taiwanese data')\n",
    "plt.plot(x, y, 'r-', lw=2, label='Fitted Gamma distribution')\n",
    "plt.fill_between(x, y_lower, y_upper, color='r', alpha=0.2, label=f'{CI}% CI')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Asymptomatic to Recovered Days')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "course_parameters = np.append(course_parameters, (alpha, scale, loc))\n",
    "course_parameters_lb = np.append(course_parameters_lb, (alpha_lb, scale_lb, loc_lb))\n",
    "course_parameters_ub = np.append(course_parameters_ub, (alpha_ub, scale_ub, loc_ub))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4450289",
   "metadata": {},
   "source": [
    "### 2.1.6 Fit Gamma distribution to days symptomatic to critically ill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(symptom_to_icu_days) != np.ndarray:\n",
    "    symptom_to_icu_days = symptom_to_icu_days.apply(lambda x: x.days).to_numpy()\n",
    "# Fit Gamma distribution to days symptomatic to confirmed\n",
    "# symptom_to_icu_days = symptom_to_icu_days[symptom_to_icu_days!=0] # To avoid floc=0 error, I ignore the 0 in the array.\n",
    "print('Number of cases:', len(symptom_to_icu_days))\n",
    "\n",
    "alpha, alpha_lb, alpha_ub, loc, loc_lb, loc_ub, scale, scale_lb, scale_ub = gamma_fit_bootstrap(symptom_to_icu_days, CI, allow_negative=True)\n",
    "symptom_to_icu_shape = alpha\n",
    "symptom_to_icu_scale = scale\n",
    "symptom_to_icu_loc = loc\n",
    "\n",
    "print(f\"alpha: {alpha:.4f} ({alpha_lb:.4f}, {alpha_ub:.4f})\")\n",
    "print(f\"loc: {loc:.4f} ({loc_lb:.4f}, {loc_ub:.4f})\")\n",
    "print(f\"scale: {scale:.4f} ({scale_lb:.4f}, {scale_ub:.4f})\")\n",
    "\n",
    "# Plot the fitted distribution with confidence interval\n",
    "x = np.linspace(0, 80, 1000)\n",
    "y = stats.gamma.pdf(x, alpha, loc, scale)\n",
    "\n",
    "# Calculate confidence intervals\n",
    "y_lower = stats.gamma.pdf(x, alpha_lb, loc_lb, scale_lb)\n",
    "y_upper = stats.gamma.pdf(x, alpha_ub, loc_ub, scale_ub)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(symptom_to_icu_days, density=True, alpha=0.7, label='Taiwanese data')\n",
    "plt.plot(x, y, 'r-', lw=2, label='Fitted Gamma distribution')\n",
    "plt.fill_between(x, y_lower, y_upper, color='r', alpha=0.2, label=f'{CI}% CI')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Symptomatic to Critically illed Days')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "course_parameters = np.append(course_parameters, (alpha, scale, loc))\n",
    "course_parameters_lb = np.append(course_parameters_lb, (alpha_lb, scale_lb, loc_lb))\n",
    "course_parameters_ub = np.append(course_parameters_ub, (alpha_ub, scale_ub, loc_ub))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5f0055",
   "metadata": {},
   "source": [
    "### 2.1.7 Fit Gamma distribution to days symptomatic to recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0bb302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 24 data points. We just use the Gamma fit\n",
    "\n",
    "if  type(symptom_to_recover_days) != np.ndarray:\n",
    "    symptom_to_recover_days = symptom_to_recover_days.apply(lambda x: x.days).to_numpy()\n",
    "\n",
    "print('Number of cases:', len(symptom_to_recover_days))\n",
    "print()\n",
    "\n",
    "alpha, loc, scale = stats.gamma.fit(symptom_to_recover_days)\n",
    "# alpha, alpha_lb, alpha_ub, loc, loc_lb, loc_ub, scale, scale_lb, scale_ub = gamma_fit_bootstrap(symptom_to_recover_days, CI)\n",
    "alpha_lb = alpha*0.8\n",
    "alpha_ub = alpha*1.2\n",
    "loc_lb = loc*0.8\n",
    "loc_ub = loc*1.1\n",
    "scale_lb = scale*0.8\n",
    "scale_ub = scale*1.2\n",
    "\n",
    "symptom_to_recover_shape = alpha\n",
    "symptom_to_recover_scale = scale\n",
    "symptom_to_recover_loc = loc\n",
    "\n",
    "print(f\"alpha: {alpha:.4f} ({alpha_lb:.4f}, {alpha_ub:.4f})\")\n",
    "print(f\"loc: {loc:.4f} ({loc_lb:.4f}, {loc_ub:.4f})\")\n",
    "print(f\"scale: {scale:.4f} ({scale_lb:.4f}, {scale_ub:.4f})\")\n",
    "\n",
    "# Plot the fitted distribution with confidence interval\n",
    "x = np.linspace(0, 80, 1000)\n",
    "y = stats.gamma.pdf(x, alpha, loc, scale)\n",
    "\n",
    "# Calculate confidence intervals\n",
    "y_lower = stats.gamma.pdf(x, alpha_lb, loc_lb, scale_lb)\n",
    "y_upper = stats.gamma.pdf(x, alpha_ub, loc_ub, scale_ub)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(symptom_to_recover_days, density=True, alpha=0.7, label='Taiwanese data')\n",
    "plt.plot(x, y, 'r-', lw=2, label='Fitted Gamma distribution')\n",
    "plt.fill_between(x, y_lower, y_upper, color='r', alpha=0.2, label=f'{CI*100}% CI')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Symptomatic to Recovered Days')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "course_parameters = np.append(course_parameters, (alpha, scale, loc))\n",
    "course_parameters_lb = np.append(course_parameters_lb, (alpha_lb, scale_lb, loc_lb))\n",
    "course_parameters_ub = np.append(course_parameters_ub, (alpha_ub, scale_ub, loc_ub))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bd97df",
   "metadata": {},
   "source": [
    "### 2.1.8 Fit Gamma distribution to days critically ill to recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df579940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: It's only 24 data points. We just use the Gamma fit\n",
    "\n",
    "if  type(icu_to_recover_days) != np.ndarray:\n",
    "    icu_to_recover_days = icu_to_recover_days.apply(lambda x: x.days).to_numpy()\n",
    "\n",
    "print('Number of cases:', len(icu_to_recover_days))\n",
    "print()\n",
    "\n",
    "alpha, loc, scale = stats.gamma.fit(icu_to_recover_days)\n",
    "# alpha, alpha_lb, alpha_ub, loc, loc_lb, loc_ub, scale, scale_lb, scale_ub = gamma_fit_bootstrap(icu_to_recover_days, CI)\n",
    "alpha_lb = alpha*0.8\n",
    "alpha_ub = alpha*1.2\n",
    "loc_lb = loc*0.8\n",
    "loc_ub = loc*1.2\n",
    "scale_lb = scale*0.8\n",
    "scale_ub = scale*1.2\n",
    "\n",
    "icu_to_recover_shape = alpha\n",
    "icu_to_recover_scale = scale\n",
    "icu_to_recover_loc = loc\n",
    "\n",
    "print(f\"alpha: {alpha:.4f} ({alpha_lb:.4f}, {alpha_ub:.4f})\")\n",
    "print(f\"loc: {loc:.4f} ({loc_lb:.4f}, {loc_ub:.4f})\")\n",
    "print(f\"scale: {scale:.4f} ({scale_lb:.4f}, {scale_ub:.4f})\")\n",
    "\n",
    "# Plot the fitted distribution with confidence interval\n",
    "x = np.linspace(0, 80, 1000)\n",
    "y = stats.gamma.pdf(x, alpha, loc, scale)\n",
    "\n",
    "# Calculate confidence intervals\n",
    "y_lower = stats.gamma.pdf(x, alpha_lb, loc_lb, scale_lb)\n",
    "y_upper = stats.gamma.pdf(x, alpha_ub, loc_ub, scale_ub)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(icu_to_recover_days, density=True, alpha=0.7, label='Taiwanese data')\n",
    "plt.plot(x, y, 'r-', lw=2, label='Fitted Gamma distribution')\n",
    "plt.fill_between(x, y_lower, y_upper, color='r', alpha=0.2, label=f'{CI*100}% CI')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Critically to Recovered Days')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "course_parameters = np.append(course_parameters, (alpha, scale, loc))\n",
    "course_parameters_lb = np.append(course_parameters_lb, (alpha_lb, scale_lb, loc_lb))\n",
    "course_parameters_ub = np.append(course_parameters_ub, (alpha_ub, scale_ub, loc_ub))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828c1e6",
   "metadata": {},
   "source": [
    "### 2.1.9 Infection to death\n",
    "\n",
    "Parameters from \"Ward, T., & Johnsen, A. (2021). Understanding an evolving pandemic: An analysis of the clinical time delay distributions of COVID-19 in the United Kingdom. Plos one, 16(10), e0257978.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5f36d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "infection_to_death_shape = 3.07\n",
    "infection_to_death_shape_lb = 2.96\n",
    "infection_to_death_shape_ub = 3.17\n",
    "\n",
    "infection_to_death_scale = 1/0.15\n",
    "infection_to_death_scale_lb = 6.25\n",
    "infection_to_death_scale_ub = 6.67\n",
    "\n",
    "course_parameters = np.append(course_parameters, (infection_to_death_shape, infection_to_death_scale))\n",
    "course_parameters_lb = np.append(course_parameters_lb, (infection_to_death_shape_lb, infection_to_death_scale_lb))\n",
    "course_parameters_ub = np.append(course_parameters_ub, (infection_to_death_shape_ub, infection_to_death_scale_ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57596e4a",
   "metadata": {},
   "source": [
    "### 2.1.10 Fit Gamma distribution to days from negative test to positive test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "576a835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_test_date_1 = pd.to_timedelta(extract_state_data(\n",
    "    data_1_to_579, 'confirmed_date', 'negative_test_date_1'))\n",
    "negative_test_date_2 = pd.to_timedelta(extract_state_data(\n",
    "    data_1_to_579, 'confirmed_date', 'negative_test_date_2'))\n",
    "negative_test_date_3 = pd.to_timedelta(extract_state_data(\n",
    "    data_1_to_579, 'confirmed_date', 'negative_test_date_3'))\n",
    "negative_test_date_4 = pd.to_timedelta(extract_state_data(\n",
    "    data_1_to_579, 'confirmed_date', 'negative_test_date_4'))\n",
    "\n",
    "negative_test_date_array = np.hstack([negative_test_date_1.dt.days.to_numpy(), negative_test_date_2.dt.days.to_numpy(\n",
    "), negative_test_date_3.dt.days.to_numpy(), negative_test_date_4.dt.days.to_numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfb62d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of cases:', len(negative_test_date_array))\n",
    "\n",
    "alpha, alpha_lb, alpha_ub, loc, loc_lb, loc_ub, scale, scale_lb, scale_ub = gamma_fit_bootstrap(-negative_test_date_array, CI)\n",
    "negative_to_confirmed_shape = alpha\n",
    "negative_to_confirmed_scale = scale\n",
    "negative_to_confirmed_loc = loc\n",
    "\n",
    "print(f\"alpha: {alpha:.4f} ({alpha_lb:.4f}, {alpha_ub:.4f})\")\n",
    "print(f\"loc: {loc:.4f} ({loc_lb:.4f}, {loc_ub:.4f})\")\n",
    "print(f\"scale: {scale:.4f} ({scale_lb:.4f}, {scale_ub:.4f})\")\n",
    "\n",
    "# Plot the fitted distribution with confidence interval\n",
    "x = np.linspace(0, 80, 1000)\n",
    "y = stats.gamma.pdf(x, alpha, loc, scale)\n",
    "\n",
    "# Calculate confidence intervals\n",
    "y_lower = stats.gamma.pdf(x, alpha_lb, loc_lb, scale_lb)\n",
    "y_upper = stats.gamma.pdf(x, alpha_ub, loc_ub, scale_ub)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(-negative_test_date_array, density=True, alpha=0.7, label='Taiwanese data')\n",
    "plt.plot(x, y, 'r-', lw=2, label='Fitted Gamma distribution')\n",
    "plt.fill_between(x, y_lower, y_upper, color='r', alpha=0.2, label=f'{CI*100}% CI')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Negative test to Confirmed Days')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "course_parameters = np.append(course_parameters, (alpha, scale, loc))\n",
    "course_parameters_lb = np.append(course_parameters_lb, (alpha_lb, scale_lb, loc_lb))\n",
    "course_parameters_ub = np.append(course_parameters_ub, (alpha_ub, scale_ub, loc_ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43a78c6",
   "metadata": {},
   "source": [
    "### 2.1.11 Age dependent risk ratio from Cheng2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7991b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_risk_ratios = np.array([0.3, 1, 2.19, 1.75]) # For age 0-19, 20-39, 40-59, and 60 above. Note that for age 0-19, I set it to be 0.3 by 1/281\n",
    "age_risk_ratios_lb = np.array([0, 0, 0.78, 0.44])\n",
    "age_risk_ratios_ub = np.array([1, 2, 6.14, 6.97])\n",
    "\n",
    "\n",
    "course_parameters = np.append(course_parameters, age_risk_ratios)\n",
    "course_parameters_lb = np.append(course_parameters_lb, age_risk_ratios_lb)\n",
    "course_parameters_ub = np.append(course_parameters_ub, age_risk_ratios_ub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d526d2",
   "metadata": {},
   "source": [
    "### 2.1.12 Natural immunity\n",
    "\n",
    "Pilz, S., Chakeri, A., Ioannidis, J. P., Richter, L., Theiler‐Schwetz, V., Trummer, C., ... & Allerberger, F. (2021). SARS‐CoV‐2 re‐infection risk in Austria. European journal of clinical investigation, 51(4), e13520."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bb9c75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_immunity_rate = 0.91  \n",
    "natural_immunity_rate_lb = 0.87\n",
    "natural_immunity_rate_ub = 0.93\n",
    "\n",
    "course_parameters = np.append(course_parameters, natural_immunity_rate)\n",
    "course_parameters_lb = np.append(course_parameters_lb, natural_immunity_rate_lb)\n",
    "course_parameters_ub = np.append(course_parameters_ub, natural_immunity_rate_ub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990f77cb",
   "metadata": {},
   "source": [
    "### 2.1.12 Vaccine efficacy\n",
    "\n",
    "Article_Dhayfule2021_Vaccine_review_210514\n",
    "\n",
    "Figure 6: Vaccine efficacy against SARS-CoV-2 asymptomatic with 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1daecc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine_rate = 0 # randomly set\n",
    "vaccine_rate_lb = 0\n",
    "vaccine_rate_ub = 0\n",
    "\n",
    "vaccine_efficacy = 0.905\n",
    "vaccine_efficacy_lb = 0.881\n",
    "vaccine_efficacy_ub = 0.924\n",
    "\n",
    "course_parameters = np.append(course_parameters, (vaccine_rate, vaccine_efficacy))\n",
    "course_parameters_lb = np.append(course_parameters_lb, (vaccine_rate_lb, vaccine_efficacy_lb))\n",
    "course_parameters_ub = np.append(course_parameters_ub, (vaccine_rate_ub, vaccine_efficacy_ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6391c2",
   "metadata": {},
   "source": [
    "### 2.1.13 Attack rate, data extracted from Cheng2020 and Ge2021, time range -14 to 10\n",
    "\n",
    "- Workplace attack rate is from Chen2021_COVID-19 \n",
    "- Class attack rate is from Huang2021_Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f49dd290",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_rate = np.array([0.86, 0.98, 1.09, 1.16, 1.16, 1.07, 0.95, 0.84, 0.78, 0.78, 0.86,\n",
    "                       1.02, 1.18, 1.3, 1.34, 1.33, 1.27, 1.19, 1.11, 1.05, 1.01, 0.99, 0.98, 0.99, 0.99]) # Relative risk\n",
    "shift = np.mean(attack_rate) -1 # Mean of the relative risk should be 1\n",
    "attack_rate = attack_rate - shift\n",
    "attack_rate_lb = np.array([0.39, 0.63, 0.9, 0.91, 0.81, 0.75, 0.72, 0.7, 0.65, 0.63, 0.72, \n",
    "                           0.9, 1.05, 1.13, 1.18, 1.19, 1.15, 1.07, 0.98, 0.92, 0.89, 0.88, 0.84, 0.76, 0.69]) - shift\n",
    "attack_rate_ub = np.array([1.91, 1.52, 1.31, 1.48, 1.66, 1.54, 1.26, 1.02, 0.94, 0.96, 1.04,\n",
    "                           1.15, 1.34, 1.49, 1.54, 1.49, 1.4, 1.32, 1.26, 1.2, 1.14, 1.11, 1.15, 1.27, 1.43]) - shift\n",
    "# attack_rate = np.array([0.78, 0.78, 0.86,\n",
    "#                        1.02, 1.18, 1.3, 1.34, 1.33, 1.27, 1.19, 1.11, 1.05, 1.01, 0.99, 0.98, 0.99, 0.99])\n",
    "\n",
    "household_attack_rate = attack_rate*0.101\n",
    "household_attack_rate_lb = attack_rate_lb*0.101\n",
    "household_attack_rate_ub = attack_rate_ub*0.101\n",
    "\n",
    "school_attack_rate = attack_rate*0.024\n",
    "school_attack_rate_lb = attack_rate_lb*0.024\n",
    "school_attack_rate_lb = school_attack_rate_lb * (0.01 / np.mean(school_attack_rate_lb))\n",
    "school_attack_rate_ub = attack_rate_ub*0.024\n",
    "school_attack_rate_ub = school_attack_rate_ub * (0.04 / np.mean(school_attack_rate_ub))\n",
    "\n",
    "workplace_attack_rate = attack_rate*0.034\n",
    "workplace_attack_rate_lb = attack_rate_lb*0.034\n",
    "workplace_attack_rate_lb = workplace_attack_rate_lb * (0.01 / np.mean(workplace_attack_rate_lb))\n",
    "workplace_attack_rate_ub = attack_rate_ub*0.034\n",
    "workplace_attack_rate_ub = workplace_attack_rate_ub * (0.15 / np.mean(workplace_attack_rate_ub))\n",
    "\n",
    "health_care_attack_rate = attack_rate*0.004\n",
    "health_care_attack_rate_lb = attack_rate_lb*0.004\n",
    "health_care_attack_rate_lb = health_care_attack_rate_lb * (0.001 / np.mean(health_care_attack_rate_lb))\n",
    "health_care_attack_rate_ub = attack_rate_ub*0.004\n",
    "health_care_attack_rate_ub = health_care_attack_rate_ub * (0.016 / np.mean(health_care_attack_rate_ub))\n",
    "\n",
    "municipality_attack_rate = (attack_rate/np.mean(attack_rate))*0.002\n",
    "municipality_attack_rate_lb = (attack_rate_lb/np.mean(attack_rate))*0.002\n",
    "municipality_attack_rate_lb = municipality_attack_rate_lb * (0.001 / np.mean(municipality_attack_rate_lb))\n",
    "municipality_attack_rate_ub = (attack_rate_ub/np.mean(attack_rate))*0.002\n",
    "municipality_attack_rate_ub = municipality_attack_rate_ub * (0.01 / np.mean(municipality_attack_rate_ub))\n",
    "\n",
    "course_parameters = np.append(course_parameters, (household_attack_rate, school_attack_rate, workplace_attack_rate, \n",
    "                                                  health_care_attack_rate, municipality_attack_rate))\n",
    "course_parameters_lb = np.append(course_parameters_lb, (household_attack_rate_lb, school_attack_rate_lb, workplace_attack_rate_lb, \n",
    "                                                        health_care_attack_rate_lb, municipality_attack_rate_lb))\n",
    "course_parameters_ub = np.append(course_parameters_ub, (household_attack_rate_ub, school_attack_rate_ub, workplace_attack_rate_ub, \n",
    "                                                        health_care_attack_rate_ub, municipality_attack_rate_ub))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa3d21a7",
   "metadata": {},
   "source": [
    "## 2.2 Transition probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3431efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_percentage = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "70c99d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "infection_to_recovered_transition_p = 0.09 # Cheng et al.\n",
    "symptom_to_recovered_transition_p = 1-0.18 # https://www.cna.com.tw/news/ahel/202210285003.aspx, https://www.thenewslens.com/article/154421, Taiwan_critically_ill_rate.png\n",
    "critically_ill_to_recovered_transition_p = len(icu_to_recover_days)/(\n",
    "    len(icu_to_recover_days)+len(icu_to_dead_days))\n",
    "\n",
    "transition_p = np.array([infection_to_recovered_transition_p, symptom_to_recovered_transition_p,\n",
    "                        critically_ill_to_recovered_transition_p])\n",
    "transition_p_lb = transition_p-transition_p*shift_percentage\n",
    "transition_p_lb[transition_p_lb<0] = 0\n",
    "transition_p_ub = transition_p+transition_p*shift_percentage\n",
    "transition_p_ub[transition_p_ub>1] = 1\n",
    "\n",
    "course_parameters = np.hstack([course_parameters, transition_p])\n",
    "course_parameters_lb = np.hstack([course_parameters_lb, transition_p_lb])\n",
    "course_parameters_ub = np.hstack([course_parameters_ub, transition_p_ub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51129e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Infection to recovered: {infection_to_recovered_transition_p: .4f}')\n",
    "print(f'Symptom to recovered: {symptom_to_recovered_transition_p: .4f}')\n",
    "print(f'Critically ill to recovered: {critically_ill_to_recovered_transition_p: .4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7764bbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transition_p_lb)\n",
    "print(transition_p_ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2148ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if course_parameters stay within course_parameters_lb and course_parameters_ub\n",
    "within_bounds = np.logical_and(course_parameters >= course_parameters_lb, \n",
    "                               course_parameters <= course_parameters_ub)\n",
    "\n",
    "if np.all(within_bounds):\n",
    "    print(\"All course_parameters are within their respective bounds.\")\n",
    "else:\n",
    "    out_of_bounds = np.where(~within_bounds)[0]\n",
    "    print(f\"Warning: {len(out_of_bounds)} parameter(s) are out of bounds:\")\n",
    "    for idx in out_of_bounds:\n",
    "        print(f\"Parameter {idx}: {course_parameters[idx]:.4f} \"\n",
    "              f\"(bounds: {course_parameters_lb[idx]:.4f}, {course_parameters_ub[idx]:.4f})\")\n",
    "\n",
    "# Visualize the parameters and their bounds\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(course_parameters))\n",
    "plt.errorbar(x, course_parameters, \n",
    "             yerr=[course_parameters - course_parameters_lb, \n",
    "                   course_parameters_ub - course_parameters],\n",
    "             fmt='.', capsize=5, capthick=2, ecolor='red', label='Bounds')\n",
    "plt.plot(x, course_parameters, 'b-', label='Parameters')\n",
    "plt.xlabel('Parameter Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Course Parameters with Bounds')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5ed6ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_p_state:\n",
    "    with open('./variable/course_parameters.npy', 'wb') as f:\n",
    "        np.save(f, course_parameters)\n",
    "    with open('./variable/course_parameters_lb.npy', 'wb') as f:\n",
    "        np.save(f, course_parameters_lb)\n",
    "    with open('./variable/course_parameters_ub.npy', 'wb') as f:\n",
    "        np.save(f, course_parameters_ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f9553",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(course_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8093b237",
   "metadata": {},
   "source": [
    "# 3. Social contact probability boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2928dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_parameters = {\n",
    "    'household_lower_bound': [0.95, 0.4, 0.05, 0.01, 1, 0, 0],\n",
    "    'household_upper_bound': [1, 1, 0.5, 0.1, 20, 10, 10],\n",
    "\n",
    "    'school_lower_bound': [0.1, 0.5, 0.01, 0.001, 1, 0, 0],\n",
    "    'school_upper_bound': [1, 1, 0.5, 0.05, 10, 10, 10],\n",
    "\n",
    "    'workplace_lower_bound': [0.1, 0.5, 0.01, 0.001, 1, 0, 0],\n",
    "    'workplace_upper_bound': [1, 1, 0.5, 0.05, 10, 10, 10],\n",
    "\n",
    "    'health_care_lower_bound': [0.005, 0.5, 0.001, 0.6, 1, 0, 0],\n",
    "    'health_care_upper_bound': [0.008, 1, 0.006, 1, 10, 10, 10],\n",
    "\n",
    "    'municipality_lower_bound': [0.000002, 0.5, 0.01, 0.001, 1, 0, 0],\n",
    "    'municipality_upper_bound': [0.00001, 1, 0.1, 0.05, 10, 10, 10],\n",
    "\n",
    "    'overdispersion_lower_bound': [0, 1],\n",
    "    'overdispersion_upper_bound': [0.2, 20]\n",
    "}\n",
    "\n",
    "if save_p_state:\n",
    "    with open('./variable/contact_parameters.pkl', 'wb') as f:\n",
    "        pickle.dump(contact_parameters, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98787c93",
   "metadata": {},
   "source": [
    "# 4. Firefly results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c067b36f",
   "metadata": {},
   "source": [
    "## 4.1 Load optimization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e083f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = Path('./firefly_result/Firefly_result_pop_size_100_alpha_1_betamin_1_gamma_0.131_max_generations_200')\n",
    "file_name = 'firefly_best.txt'\n",
    "\n",
    "# Load best fireflies\n",
    "with open(folder/file_name, 'r') as f:\n",
    "    best_results = [[float(num) for num in line.split(' ')] for line in f]\n",
    "\n",
    "best_results = np.array(best_results)\n",
    "# best_iterations = best_results[:, 0]\n",
    "fireflies = best_results[:, 1:-1]\n",
    "costs = best_results[:, -1]\n",
    "\n",
    "# Get the 10% best fireflies\n",
    "top_10_percent = int(len(costs)*0.1)\n",
    "sorted_indices = np.argsort(costs)\n",
    "best_cost_index = sorted_indices[:top_10_percent]\n",
    "\n",
    "best_costs = costs[best_cost_index]\n",
    "best_fireflies = fireflies[best_cost_index, :]\n",
    "\n",
    "\n",
    "# Load bound\n",
    "file_name = 'bound.txt'\n",
    "# file_name = 'C:/Users/chee8/Desktop/to_be_deleted/bound.txt'\n",
    "with open(folder/file_name, 'r') as f:\n",
    "    bounds = [[float(num) for num in line.split(' ')] for line in f]\n",
    "bounds = np.array(bounds)\n",
    "lower_bound = bounds[0, :]\n",
    "upper_bound = bounds[1, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec5fba9",
   "metadata": {},
   "source": [
    "## 4.2 Firefly results of the social contact probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0513ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude code\n",
    "# Social contact probability\n",
    "social_contact_firefly_result_dict = {}\n",
    "social_contact_firefly_lower_bound_dict = {}\n",
    "social_contact_firefly_upper_bound_dict = {}\n",
    "contact_layers = ['Household', 'School', 'Workplace', 'Heath care', 'Municipality']\n",
    "parameters = [\n",
    "    'contact probability',\n",
    "    'consecutive daily contact probability',\n",
    "    'contact probability when health',\n",
    "    'contact probability when symptomatic',\n",
    "    'steepness',\n",
    "    'phase relative to symptom-onset for symptomatic',\n",
    "    'phase relative to symptomatic for resuming normal social context'\n",
    "]\n",
    "extra_parameters = ['overdispersion rate', 'overdispersion weight']\n",
    "\n",
    "pe_social_contact_p = pd.DataFrame()\n",
    "\n",
    "# Function to check if a value is within bounds\n",
    "def is_within_bounds(value, lower, upper):\n",
    "    return lower <= value <= upper\n",
    "\n",
    "# Flag to track if any firefly is out of bounds\n",
    "any_firefly_out_of_bounds = False\n",
    "\n",
    "for i in range(top_10_percent):\n",
    "    firefly_within_bounds = True\n",
    "    for contact_layer in contact_layers:\n",
    "        for param in parameters:\n",
    "            key = f'{contact_layer} {param}'\n",
    "            idx = contact_layers.index(contact_layer) * len(parameters) + parameters.index(param)\n",
    "            \n",
    "            value = best_fireflies[i, idx]\n",
    "            lower = lower_bound[idx]\n",
    "            upper = upper_bound[idx]\n",
    "            \n",
    "            if not is_within_bounds(value, lower, upper):\n",
    "                print(f\"Warning: Firefly {i+1}, {key} is out of bounds. Value: {value}, Bounds: [{lower}, {upper}]\")\n",
    "                firefly_within_bounds = False\n",
    "                any_firefly_out_of_bounds = True\n",
    "            \n",
    "            social_contact_firefly_result_dict[key] = value\n",
    "            social_contact_firefly_lower_bound_dict[key] = lower\n",
    "            social_contact_firefly_upper_bound_dict[key] = upper\n",
    "    for extra_parm in extra_parameters:\n",
    "        key = f'{extra_parm}'\n",
    "        idx += 1\n",
    "\n",
    "        value = best_fireflies[i, idx]\n",
    "        lower = lower_bound[idx]\n",
    "        upper = upper_bound[idx]\n",
    "        if not is_within_bounds(value, lower, upper):\n",
    "                print(f\"Warning: Firefly {i+1}, {key} is out of bounds. Value: {value}, Bounds: [{lower}, {upper}]\")\n",
    "                firefly_within_bounds = False\n",
    "                any_firefly_out_of_bounds = True\n",
    "            \n",
    "        social_contact_firefly_result_dict[key] = value\n",
    "        social_contact_firefly_lower_bound_dict[key] = lower\n",
    "        social_contact_firefly_upper_bound_dict[key] = upper\n",
    "    \n",
    "    if not firefly_within_bounds:\n",
    "        print(f\"Firefly {i+1} is out of bounds for one or more parameters.\")\n",
    "    \n",
    "    pe_social_contact_p[f'Optimized value {i+1}'] = pd.Series(social_contact_firefly_result_dict)\n",
    "\n",
    "pe_social_contact_p['Lower bound'] = pd.Series(social_contact_firefly_lower_bound_dict)\n",
    "pe_social_contact_p['Upper bound'] = pd.Series(social_contact_firefly_upper_bound_dict)\n",
    "\n",
    "pe_social_contact_p.to_excel('./variable/pe_social_contact_parameters.xlsx')\n",
    "\n",
    "if any_firefly_out_of_bounds:\n",
    "    print(\"Warning: One or more fireflies were found to be out of bounds.\")\n",
    "else:\n",
    "    print(\"All fireflies are within the specified boundaries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be70fb0",
   "metadata": {},
   "source": [
    "## 4.3 Firefly results of the course of disease prameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bf107b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_var_name = [\n",
    "    'Latent period, Gamma shape', 'Latent period, Gamma scale',\n",
    "    'Infectious period, Gamma shape', 'Infectious period, Gamma scale',\n",
    "    'Incubation period, Gamma shape', 'Incubation period, Gamma scale',\n",
    "    'Symptom onset to confirmed period, Gamma shape', 'Symptom onset to confirmed period, Gamma scale',\n",
    "    'Symptom onset to confirmed period, Gamma loc', 'Asymptomatic to recovered period, Gamma shape',\n",
    "    'Asymptomatic to recovered period, Gamma scale', 'Asymptomatic to recovered period, Gamma loc',\n",
    "    'Symptom onset to critically ill period, Gamma shape', 'Symptom onset to critically ill period, Gamma scale', 'Symptom onset to critically ill period, Gamma loc',\n",
    "    'Symptom onset to recovered period, Gamma shape', 'Symptom onset to recovered period, Gamma scale', 'Symptom onset to recovered period, Gamma loc',\n",
    "    'Critically ill to recovered period, Gamma shape', 'Critically ill to recovered period, Gamma scale',\n",
    "    'Critically ill to recovered period, Gamma loc', 'Asymptomatic to death period, Gamma shape',\n",
    "    'Asymptomatic to death period, Gamma scale', 'Negative test to confirmed period, Gamma shape',\n",
    "    'Negative test to confirmed period, Gamma scale', 'Negative test to confirmed period, Gamma loc'\n",
    "]\n",
    "# tmp = [f'Age risk ratio {i}' for i in range(4)]\n",
    "tmp = ['Age risk ratio, 0-19', 'Age risk ratio, 20-39', 'Age risk ratio, 40-59', 'Age risk ratio, >=60']\n",
    "course_var_name = course_var_name + tmp\n",
    "tmp = ['Natural immunity rate', 'Vaccination rate', 'Vaccine efficacy']\n",
    "course_var_name = course_var_name + tmp\n",
    "tmp = [f'Household secondary attack rate relative risk: {i} days from onset of symptom to contact date' for i in range(-14, 10+1)]\n",
    "course_var_name = course_var_name + tmp\n",
    "tmp = [f'School secondary attack rate relative risk: {i} days from onset of symptom to contact date' for i in range(-14, 10+1)]\n",
    "course_var_name = course_var_name + tmp\n",
    "tmp = [f'Workplace secondary attack rate relative risk: {i} days from onset of symptom to contact date' for i in range(-14, 10+1)]\n",
    "course_var_name = course_var_name + tmp\n",
    "tmp = [f'Health care secondary attack rate relative risk: {i} days from onset of symptom to contact date' for i in range(-14, 10+1)]\n",
    "course_var_name = course_var_name + tmp\n",
    "tmp = [f'Municipality secondary attack rate relative risk: {i} days from onset of symptom to contact date' for i in range(-14, 10+1)]\n",
    "course_var_name = course_var_name + tmp\n",
    "tmp = ['Infection to recovered transition p', 'Symptomatic to recovered transition p',\n",
    "       'Critically ill to recovered transition p']\n",
    "course_var_name = course_var_name + tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "62899bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_course_of_disease_p = pd.DataFrame()\n",
    "for i in range(len(best_fireflies)):\n",
    "    # Assign value\n",
    "    course_firefly_result_dict = dict(zip(course_var_name, best_fireflies[i, 37:]))\n",
    "    pe_course_of_disease_p[f'Optimized value {i+1}'] = pd.DataFrame.from_dict(course_firefly_result_dict, orient='index', columns=[f'Optimized value {i+1}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "82e1b0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_firefly_lower_bound_dict = dict(zip(course_var_name, lower_bound[37::]))\n",
    "course_firefly_upper_bound_dict = dict(zip(course_var_name, upper_bound[37::]))\n",
    "pe_course_of_disease_p['Lower bound'] = pd.DataFrame.from_dict(course_firefly_lower_bound_dict, orient='index', columns=['Lower bound'])\n",
    "pe_course_of_disease_p['Upper bound'] = pd.DataFrame.from_dict(course_firefly_upper_bound_dict, orient='index', columns=['Upper bound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aad14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the initial value and the corresponding reference\n",
    "pe_course_of_disease_p['Initial value'] = '-'\n",
    "pe_course_of_disease_p['Reference 1'] = '-'\n",
    "pe_course_of_disease_p['Reference 2'] = '-'\n",
    "init_course_of_disease_p = pd.read_excel(Path('./variable/init_course_of_disease_parameters.xlsx'), sheet_name=0)\n",
    "layer_attack_rate = [10.1/(100*np.mean(attack_rate)), 2.4/(100*np.mean(attack_rate)), \n",
    "                    3.4/(100*np.median(attack_rate)), 0.4/(100*np.mean(attack_rate)), \n",
    "                    0.2/(100*np.mean(attack_rate))]\n",
    "\n",
    "layers = ['Household', 'School', 'Workplace', 'Health care', 'Municipality']\n",
    "for i in range(len(init_course_of_disease_p)):\n",
    "    variable = init_course_of_disease_p.loc[i, 'Variable']\n",
    "    if variable in pe_course_of_disease_p.index:\n",
    "        pe_course_of_disease_p.loc[variable, 'Initial value'] = init_course_of_disease_p.loc[i, 'Value']\n",
    "        pe_course_of_disease_p.loc[variable, 'Reference 1'] = init_course_of_disease_p.loc[i, 'Reference']\n",
    "    print()\n",
    "    if variable[:9] == 'Secondary':\n",
    "        for j, layer in enumerate(layers):\n",
    "            tmp = layer + ' ' + variable.lower()\n",
    "            pe_course_of_disease_p.loc[tmp, 'Initial value'] = init_course_of_disease_p.loc[i, 'Value']*layer_attack_rate[j]\n",
    "            pe_course_of_disease_p.loc[tmp, 'Reference 1'] = init_course_of_disease_p.loc[i, 'Reference']\n",
    "            if layer == 'Household':\n",
    "                pe_course_of_disease_p.loc[tmp, 'Reference 2'] = 'Ge, Y., Martinez, L., Sun, S., Chen, Z., Zhang, F., Li, F., Sun, W., Chen, E., Pan, J., Li, C., et al.: Covid-19 transmission dynamics among close contacts of index patients with covid-19: a population-based cohort study in zhejiang province, china. JAMA Internal Medicine 181(10), 1343–1350 (2021)'\n",
    "            elif layer == 'School':\n",
    "                pe_course_of_disease_p.loc[tmp, 'Reference 2'] = 'Huang, Y.-T., Tu, Y.-K., Lai, P.-C.: Estimation of the secondary attack rate of covid-19 using proportional meta-analysis of nationwide contact tracing data in taiwan. Journal of Microbiology, Immunology and Infection 54(1), 89–92 (2021)'\n",
    "            elif layer == 'Workplace':\n",
    "                pe_course_of_disease_p.loc[tmp, 'Reference 2'] = 'Chen, Yiqun, et al. \"COVID-19 outbreak rates and infection attack rates associated with the workplace: a descriptive epidemiological study.\" BMJ open 12.7 (2022): e055643.'\n",
    "            elif layer == 'Health care':\n",
    "                pe_course_of_disease_p.loc[tmp, 'Reference 2'] = 'Ge, Y., Martinez, L., Sun, S., Chen, Z., Zhang, F., Li, F., Sun, W., Chen, E., Pan, J., Li, C., et al.: Covid-19 transmission dynamics among close contacts of index patients with covid-19: a population-based cohort study in zhejiang province, china. JAMA Internal Medicine 181(10), 1343–1350 (2021)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "faf50431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the fitted Gamma distribution parameters to the table\n",
    "pe_course_of_disease_p.loc['Symptom onset to confirmed period, Gamma shape', 'Initial value'] = symptom_to_confirmed_shape\n",
    "pe_course_of_disease_p.loc['Symptom onset to confirmed period, Gamma shape', 'Reference 1'] = 'Determined through TW data'\n",
    "pe_course_of_disease_p.loc['Symptom onset to confirmed period, Gamma scale', 'Initial value'] = symptom_to_confirmed_scale\n",
    "pe_course_of_disease_p.loc['Symptom onset to confirmed period, Gamma scale', 'Reference 1'] = 'Determined through TW data'\n",
    "pe_course_of_disease_p.loc['Symptom onset to confirmed period, Gamma loc', 'Initial value'] = symptom_to_confirmed_loc\n",
    "pe_course_of_disease_p.loc['Symptom onset to confirmed period, Gamma loc', 'Reference 1'] = 'Determined through TW data'\n",
    "\n",
    "pe_course_of_disease_p.loc['Asymptomatic to recovered period, Gamma shape', 'Initial value'] = asymptomatic_to_recovered_shape\n",
    "pe_course_of_disease_p.loc['Asymptomatic to recovered period, Gamma shape', 'Reference 1'] = 'Determined through TW data'\n",
    "pe_course_of_disease_p.loc['Asymptomatic to recovered period, Gamma scale', 'Initial value'] = asymptomatic_to_recovered_scale\n",
    "pe_course_of_disease_p.loc['Asymptomatic to recovered period, Gamma scale', 'Reference 1'] = 'Determined through TW data'\n",
    "pe_course_of_disease_p.loc['Asymptomatic to recovered period, Gamma loc', 'Initial value'] = asymptomatic_to_recovered_loc\n",
    "pe_course_of_disease_p.loc['Asymptomatic to recovered period, Gamma loc', 'Reference 1'] = 'Determined through TW data'\n",
    "\n",
    "pe_course_of_disease_p.loc['Symptom onset to critically ill period, Gamma shape', 'Initial value'] = symptom_to_icu_shape\n",
    "pe_course_of_disease_p.loc['Symptom onset to critically ill period, Gamma shape', 'Reference 1'] = 'Determined through TW data'\n",
    "pe_course_of_disease_p.loc['Symptom onset to critically ill period, Gamma scale', 'Initial value'] = symptom_to_icu_scale\n",
    "pe_course_of_disease_p.loc['Symptom onset to critically ill period, Gamma scale', 'Reference 1'] = 'Determined through TW data'\n",
    "pe_course_of_disease_p.loc['Symptom onset to critically ill period, Gamma loc', 'Initial value'] = symptom_to_icu_loc\n",
    "pe_course_of_disease_p.loc['Symptom onset to critically ill period, Gamma loc', 'Reference 1'] = 'Determined through TW data'\n",
    "\n",
    "\n",
    "pe_course_of_disease_p.loc['Symptom onset to recovered period, Gamma shape', 'Initial value'] = symptom_to_recover_shape\n",
    "pe_course_of_disease_p.loc['Symptom onset to recovered period, Gamma shape', 'Reference 1'] = 'Determined through TW data'\n",
    "pe_course_of_disease_p.loc['Symptom onset to recovered period, Gamma scale', 'Initial value'] = symptom_to_recover_scale\n",
    "pe_course_of_disease_p.loc['Symptom onset to recovered period, Gamma scale', 'Reference 1'] = 'Determined through TW data'\n",
    "pe_course_of_disease_p.loc['Symptom onset to recovered period, Gamma loc', 'Initial value'] = symptom_to_recover_loc\n",
    "pe_course_of_disease_p.loc['Symptom onset to recovered period, Gamma loc', 'Reference 1'] = 'Determined through TW data'\n",
    "\n",
    "pe_course_of_disease_p.loc['Critically ill to recovered period, Gamma shape', 'Initial value'] = icu_to_recover_shape\n",
    "pe_course_of_disease_p.loc['Critically ill to recovered period, Gamma shape', 'Reference 1'] = 'Determined through TW data'\n",
    "pe_course_of_disease_p.loc['Critically ill to recovered period, Gamma scale', 'Initial value'] = icu_to_recover_scale\n",
    "pe_course_of_disease_p.loc['Critically ill to recovered period, Gamma scale', 'Reference 1'] = 'Determined through TW data'\n",
    "pe_course_of_disease_p.loc['Critically ill to recovered period, Gamma loc', 'Initial value'] = icu_to_recover_loc\n",
    "pe_course_of_disease_p.loc['Critically ill to recovered period, Gamma loc', 'Reference 1'] = 'Determined through TW data'\n",
    "\n",
    "pe_course_of_disease_p.loc['Negative test to confirmed period, Gamma shape', 'Initial value'] = negative_to_confirmed_shape\n",
    "pe_course_of_disease_p.loc['Negative test to confirmed period, Gamma shape', 'Reference 1'] = 'Determined through TW data'\n",
    "pe_course_of_disease_p.loc['Negative test to confirmed period, Gamma scale', 'Initial value'] = negative_to_confirmed_scale\n",
    "pe_course_of_disease_p.loc['Negative test to confirmed period, Gamma scale', 'Reference 1'] = 'Determined through TW data'\n",
    "pe_course_of_disease_p.loc['Negative test to confirmed period, Gamma loc', 'Initial value'] = negative_to_confirmed_loc\n",
    "pe_course_of_disease_p.loc['Negative test to confirmed period, Gamma loc', 'Reference 1'] = 'Determined through TW data'\n",
    "\n",
    "\n",
    "pe_course_of_disease_p.loc['Infection to recovered transition p', 'Initial value'] = infection_to_recovered_transition_p\n",
    "pe_course_of_disease_p.loc['Infection to recovered transition p', 'Reference 1'] = 'Cheng, H.-Y., Jian, S.-W., Liu, D.-P., Ng, T.-C., Huang, W.-T., Lin, H.-H.: Contact Tracing Assessment of COVID-19 Transmission Dynamics in Taiwan and Risk at Diff erent Exposure Periods Before and After Symptom Onset. JAMA Internal Medicine 180(9), 1156 (2020).'\n",
    "pe_course_of_disease_p.loc['Symptomatic to recovered transition p', 'Initial value'] = symptom_to_recovered_transition_p\n",
    "pe_course_of_disease_p.loc['Symptomatic to recovered transition p', 'Reference 1'] = 'https://www.cna.com.tw/news/ahel/202210285003.aspx'\n",
    "pe_course_of_disease_p.loc['Symptomatic to recovered transition p', 'Reference 2'] = 'https://www.thenewslens.com/article/154421'\n",
    "pe_course_of_disease_p.loc['Critically ill to recovered transition p', 'Initial value'] = critically_ill_to_recovered_transition_p\n",
    "pe_course_of_disease_p.loc['Critically ill to recovered transition p', 'Reference 1'] = 'Determined through TW data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e4b34321",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_course_of_disease_p.to_excel('./variable/pe_course_of_disease_parameters.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5293f9e",
   "metadata": {},
   "source": [
    "# 5. Taiwan data test matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff8409a",
   "metadata": {},
   "source": [
    "## 5.1 Convert Taiwan individual data to test matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "45746ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = convert_Taiwan_data_to_test_matrix(data_1_to_579)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50cf904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check non-NaN counts\n",
    "value_counts = np.sum(~np.isnan(data_matrix), axis=1)\n",
    "plt.hist(value_counts, bins=range(max(value_counts)+2))\n",
    "plt.xlabel('Number of non-NaN elements each subject in the Taiwan data matrix')\n",
    "plt.ylabel('Frequency')\n",
    "# print(f'{np.unique(value_counts, return_counts=True)}')\n",
    "print('number of non-nan element in each column: ', np.sum(~np.isnan(data_matrix), axis=0))\n",
    "\n",
    "\n",
    "# plt.savefig('Taiwan_matrix_non_nan_RW2022.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6782c91d",
   "metadata": {},
   "source": [
    "## 5.2 Contact network\n",
    "\n",
    "Check plot_taiwan_data.ipynb to see detail code for constructing the contact network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6622fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b2769687",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = pd.read_excel(data_path/'figshare_taiwan_covid.xlsx', \n",
    "                                  sheet_name='Edge List')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebd1809",
   "metadata": {},
   "source": [
    "### 5.2.1 Uninfected contacts for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e5d9243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_ids = np.arange(579, 0, -1)\n",
    "# case_ids = np.arange(579, 570, -1)\n",
    "remove_id = 530\n",
    "number_of_contacts = np.array([])\n",
    "number_of_infected_contacts = np.array([])\n",
    "for case_id in case_ids:\n",
    "    if case_id == remove_id:\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            contact_nodes = list(taiwan_contact_network['I'+str(case_id)])\n",
    "        except:\n",
    "            contact_nodes = []\n",
    "        number_of_contact = len(contact_nodes)\n",
    "        number_of_contacts = np.append(number_of_contacts, number_of_contact)\n",
    "        number_of_infected_contact = np.sum([True if type(i) == str and i[0]=='I' else False for i in contact_nodes])\n",
    "        number_of_infected_contacts = np.append(number_of_infected_contacts, number_of_infected_contact)\n",
    "\n",
    "number_of_uninfected_contacts = number_of_contacts - number_of_infected_contacts\n",
    "number_of_uninfected_contacts[number_of_uninfected_contacts==0] = np.nan # When it is 0, it \n",
    "# might because the CDC didn't release the data so that we should set it to nan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c165bf",
   "metadata": {},
   "source": [
    "### 5.2.2 Effective contacts for each subject\n",
    "\n",
    "Note that number_of_effective_contacts is different from number_of_infected_contact. Number_of_effective_contacts is focusing on the contact network with clear infection path while number_of_infected_contact is only showing the connection within infected cases. Some time the connection within the infected cases is unclear because CDC only show the infection cluster so we don't know how many people are infected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d768ac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_ids = np.arange(579, 0, -1)\n",
    "# case_ids = np.arange(579, 570, -1)\n",
    "remove_id = 530\n",
    "number_of_effective_contacts = np.array([])\n",
    "for case_id in case_ids:\n",
    "    if case_id == remove_id:\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            contact_nodes = list(infection_contact_network['I'+str(case_id)])\n",
    "        except:\n",
    "            contact_nodes = []\n",
    "        number_of_effective_contact = len(contact_nodes)\n",
    "        number_of_effective_contacts = np.append(number_of_effective_contacts, number_of_effective_contact)\n",
    "\n",
    "number_of_effective_contacts[number_of_effective_contacts==0] = np.nan      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae84a8",
   "metadata": {},
   "source": [
    "### 5.2.3 Append `number_of_uninfected_contacts` and `number_of_effective_contacts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1781b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = np.hstack([data_matrix, number_of_uninfected_contacts.reshape(-1, 1)])\n",
    "data_matrix = np.hstack([data_matrix, number_of_effective_contacts.reshape(-1, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c482c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove gender (because our model has no parameter that affect gender) and days of infection to recovered (because there were only 2 cases)\n",
    "data_matrix = np.delete(data_matrix, (1, 3), 1)\n",
    "# Remove number_of_uninfected_contacts because the data provided by Taiwan CDC is actually not number of uninfected contacts but size of the cluster.\n",
    "data_matrix = np.delete(data_matrix, -2, axis=1)\n",
    "\n",
    "print('number of non nan element in each column: ', np.sum(~np.isnan(data_matrix), axis=0))\n",
    "# Check non-NaN counts\n",
    "value_counts = np.sum(~np.isnan(data_matrix), axis=1)\n",
    "plt.hist(value_counts, bins=range(max(value_counts)+2))\n",
    "plt.xlabel('Number of non-NaN elements each subject in the Taiwan data matrix')\n",
    "plt.ylabel('Frequency')\n",
    "print(np.unique(value_counts, return_counts=True))\n",
    "\n",
    "# plt.savefig('Taiwan_matrix_non_nan_RW2022.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64995e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix size reduction, extract only the informative samples\n",
    "data_matrix_small = data_matrix[value_counts >= 3, :]\n",
    "print(data_matrix_small.shape)\n",
    "\n",
    "if save_p_state:\n",
    "    with open('./variable/Taiwan_data_matrix.npy', 'wb') as f:\n",
    "        np.save(f, data_matrix_small)\n",
    "\n",
    "    with open('./variable/Taiwan_data_matrix_full.npy', 'wb') as f:\n",
    "        np.save(f, data_matrix)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "981c73eafd070d074423928c764bc0ec6070d0d9756257c8a20d4d497db442af"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
