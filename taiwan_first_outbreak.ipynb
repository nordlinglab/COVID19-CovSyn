{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scoring  # the scoring module\n",
    "from datetime import date, datetime, timedelta\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as mdates  # Corrected import statement \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score, mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "from firefly_optimizer import *\n",
    "from plot_results import *\n",
    "from transition_probability_estimation import *\n",
    "from Data_synthesize import *\n",
    "from rw_data_processing import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload \n",
    "plt.style.use(r\"./rw_visualization.mplstyle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Color\n",
    "current_palette = seaborn.color_palette()\n",
    "current_palette\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Predefine the Taiwan source outbreak infection days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dates_to_days(dates):\n",
    "    sorted_dates = sorted(dates)\n",
    "    days = [0]\n",
    "    for i in range(1, len(sorted_dates)):\n",
    "        days.append((sorted_dates[i] - sorted_dates[0]).days)\n",
    "    return days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local positive cases confirmed dates\n",
    "sources_confirmed_dates = [\"2020/1/28\", \"2020/1/30\", \"2020/2/3\", \"2020/2/19\", \"2020/2/23\", \"2020/2/28\", \"2020/3/5\", \"2020/3/13\", \"2020/3/18\", \"2020/3/18\", \"2020/3/19\", \"2020/3/20\", \"2020/3/22\", \"2020/3/20\", \"2020/3/24\", \"2020/3/26\", \"2020/3/26\", \"2020/3/28\", \"2020/3/28\", \"2020/3/29\", \"2020/3/31\", \"2020/3/31\", \"2020/4/2\", \"2020/4/2\", \"2020/4/3\", \"2020/4/4\", \"2020/4/8\", \"2020/4/12\"]\n",
    "infection_dates = [datetime.strptime(date, \"%Y/%m/%d\") for date in sources_confirmed_dates]\n",
    "\n",
    "# Generate 1000 duplicate infection dates\n",
    "infection_dates_list = []\n",
    "infection_days_list = []\n",
    "for i in range(1100):\n",
    "    infection_dates_list.append(infection_dates)\n",
    "    infection_days = transform_dates_to_days(infection_dates)\n",
    "    infection_days_list.append(infection_days)\n",
    "\n",
    "with open('./variable/infection_days_list.pkl', 'wb') as f:\n",
    "    pickle.dump(infection_days_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: Please run data_synthesis.sh with mode taiwan_first_outbreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Taiwan first outbreak population-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tw_data_path = Path('./data/structured_course_of_disease_data/figshare_taiwan_covid.xlsx')\n",
    "tw_summary_data = pd.read_excel(tw_data_path, sheet_name='Summary')\n",
    "\n",
    "# First confirmed case date to the last confirmed case date\n",
    "tw_summary_data = tw_summary_data.loc[\\\n",
    "     (tw_summary_data['announce_date'] >= pd.to_datetime('2020-01-27')) &\\\n",
    "     (pd.to_datetime('2020-08-01') >= tw_summary_data['announce_date'])] \n",
    "\n",
    "# # Data time range of our structured course of disease dataset with contact tracing information \n",
    "# # (Wu, Yu-Heng, and TorbjÃ¶rn EM Nordling. \"A structured course of disease dataset with contact tracing information in Taiwan for COVID-19 modelling.\" Scientific Data 11.1 (2024): 821.)\n",
    "# tw_summary_data = tw_summary_data.loc[\\\n",
    "#      (tw_summary_data['announce_date'] >= pd.to_datetime('2020-01-21')) &\\\n",
    "#      (pd.to_datetime('2020-11-9') >= tw_summary_data['announce_date'])] \n",
    "\n",
    "# # First outbreak\n",
    "# tw_summary_data = tw_summary_data.loc[\\\n",
    "#      (tw_summary_data['announce_date'] >= pd.to_datetime('2020-02-16')) &\\\n",
    "#      (pd.to_datetime('2020-04-11') >= tw_summary_data['announce_date'])] \n",
    "\n",
    "# # One day before the first death to the end of the first outbreak\n",
    "# tw_summary_data = tw_summary_data.loc[\\\n",
    "#      (tw_summary_data['announce_date'] >= pd.to_datetime('2020-02-14')) &\\\n",
    "#      (pd.to_datetime('2020-04-11') >= tw_summary_data['announce_date'])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local local positive cases\n",
    "number_of_local_positive_case = tw_summary_data['number_of_local_positive_cases'].to_numpy()\n",
    "nan_index = np.isnan(number_of_local_positive_case)\n",
    "# Remove all NaN values\n",
    "number_of_local_positive_case = number_of_local_positive_case[~nan_index]\n",
    "number_of_local_positive_case = np.flip(number_of_local_positive_case.astype(np.int32),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local unknown source positive cases\n",
    "number_of_unknown_positive_cases = tw_summary_data['number_of_unknown_positive_cases'].\\\n",
    "    to_numpy()\n",
    "# Remove all NaN values\n",
    "number_of_unknown_positive_cases = number_of_unknown_positive_cases[~nan_index]\n",
    "number_of_unknown_positive_cases = np.flip(number_of_unknown_positive_cases.astype(int),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = tw_summary_data['announce_date'].to_numpy()\n",
    "# Remove all NaN values\n",
    "date_time = date_time[~nan_index]\n",
    "date_time = np.flip(date_time,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_death_cases = tw_summary_data['dead'].to_numpy()\n",
    "# Remove all NaN values\n",
    "number_of_death_cases = number_of_death_cases[~nan_index]\n",
    "number_of_death_cases = np.flip(number_of_death_cases.astype(int),0)\n",
    "\n",
    "# ID 19, local, confirmed date 02-03, death date 02-15\n",
    "# ID 27, local, confirmed date 02-23, death date 03-20\n",
    "# ID 34, local, confirmed date 02-28, death date 03-29\n",
    "# ID 101, abroad, confirmed date 03-19, death date 04-10\n",
    "# ID 108, abroad, confirmed date 03-19, death date 03-29\n",
    "# ID 170, abroad, confirmed date 03-23, death date 03-29\n",
    "# ID 197, abroad, confirmed date 03-24, death date 05-11\n",
    "\n",
    "# Only keep the first three cases since only they are local cases\n",
    "index = np.where(number_of_death_cases==5)[0][0]\n",
    "number_of_death_cases[index::] = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_missing_date(date, number_of_case):\n",
    "    \"\"\"\n",
    "    Insert the missing data by copying the data from the previous day.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    date: Numpy array of available date information with part of the dates missing\n",
    "    number_of_case: Numpy array of number of cases, e.g. abroad positive cases, recovered cases, or death cases\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    date_full: Numpy array of available date information with all the dates\n",
    "    number_of_case_full: Numpy array of number of cases, e.g. abroad positive cases, recovered cases, or death cases\n",
    "    \"\"\"\n",
    "    date_full = np.arange(date[0], date[-1]+np.timedelta64(1, 'D'), dtype='datetime64[D]')\n",
    "    index = 0\n",
    "    number_of_case_full = []\n",
    "    for i in date_full:\n",
    "        index = np.where(i == date)\n",
    "        if np.shape(index)[1] == 1: # Existed date\n",
    "            number_of_case_full = np.append(number_of_case_full, number_of_case[index[0][0]])\n",
    "            index_temp = index\n",
    "        elif np.shape(index)[1] == 0: # Missing date\n",
    "            number_of_case_full = np.append(number_of_case_full, number_of_case[index_temp[0][0]])\n",
    "        elif np.shape(index)[1] == 2: # Date exist but duplicate\n",
    "            number_of_case_full = np.append(number_of_case_full, number_of_case[index[0][-1]])\n",
    "            index_temp = index\n",
    "        else:\n",
    "            print('Error')\n",
    "            print(i)\n",
    "    return number_of_case_full, date_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_local_positive_case_full, date_full = insert_missing_date(date_time, number_of_local_positive_case)\n",
    "number_of_unknown_positive_cases_full, _ = insert_missing_date(date_time, number_of_unknown_positive_cases)\n",
    "number_of_death_cases_full, _ = insert_missing_date(date_time, number_of_death_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_tw_local_confirmed_cases = np.diff(number_of_local_positive_case_full) + np.diff(number_of_unknown_positive_cases_full)\n",
    "daily_tw_local_deaths = np.diff(number_of_death_cases_full)\n",
    "\n",
    "# Padding to make all arrays 365 days long\n",
    "daily_tw_local_confirmed_cases = np.pad(daily_tw_local_confirmed_cases, (0, 365 - len(daily_tw_local_confirmed_cases)), 'constant')\n",
    "daily_tw_local_deaths = np.pad(daily_tw_local_deaths, (0, 365 - len(daily_tw_local_deaths)), 'constant')\n",
    "daily_tw_date_time = np.arange(date_full[0], date_full[0] + np.timedelta64(365, 'D'), dtype='datetime64[D]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"The symptom onset date of the first patient identified was Dec 1, 2019.\" Huang, Chaolin, et al. \"Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China.\" The lancet 395.10223 (2020): 497-506.\n",
    "print('Largest possible time shift: ', daily_tw_date_time[0]-np.datetime64('2019-12-01'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load CovSyn data and transforme it to population-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('./synthetic_data_results_taiwan_first_outbreak')\n",
    "# Load 1000 data with at least 1 spreads happened.\n",
    "demographic_data_list_all, social_data_list_all, course_of_disease_data_list_all, contact_data_list_all, transmission_digraph_all = \\\n",
    "    load_synthetic_data(data_path, return_len=1000, memory_limit=1e9, min_case_num=len(sources_confirmed_dates)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_limit = 365\n",
    "data_len = len(demographic_data_list_all)\n",
    "daily_infected_cases_matrix = np.zeros((data_len, time_limit))\n",
    "daily_confirmed_cases_matrix = np.zeros((data_len, time_limit))\n",
    "daily_recoverd_cases_matrix = np.zeros((data_len, time_limit))\n",
    "daily_deaths_matrix = np.zeros((data_len, time_limit))\n",
    "for i in range(data_len):\n",
    "    course_of_disease_data = course_of_disease_data_list_all[i]\n",
    "    contact_data = contact_data_list_all[i]\n",
    "    daily_susceptible_population, daily_infected_cases, daily_contagious_cases, daily_symptomatic_cases, \\\n",
    "                daily_confirmed_cases, daily_tested_cases, daily_suspected_cases, daily_isolation_cases, daily_critically_ill_cases, daily_recovered_cases, \\\n",
    "                daily_deaths = transform_course_object_to_population_data(course_of_disease_data,\n",
    "                                                                            contact_data,\n",
    "                                                                            time_limit=time_limit-1,\n",
    "                                                                            population_size=23008366)\n",
    "    daily_infected_cases_matrix[i, :] = daily_infected_cases\n",
    "    daily_confirmed_cases_matrix[i, :] = daily_confirmed_cases\n",
    "    daily_recoverd_cases_matrix[i, :] = daily_recovered_cases\n",
    "    daily_deaths_matrix[i, :] = daily_deaths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create daily source infected cases array. This store the pattern of the source infection cases which I set in the CovSyn simulation.\n",
    "np_source_confirmed_dates = np.zeros(len(sources_confirmed_dates), dtype='datetime64[D]')\n",
    "for i, date_str in enumerate(sources_confirmed_dates):\n",
    "    confirm_date = np.datetime64(datetime.strptime(date_str, '%Y/%m/%d').strftime('%Y-%m-%d'))\n",
    "    np_source_confirmed_dates[i] = confirm_date\n",
    "\n",
    "min_confirmed_date = np.min(np_source_confirmed_dates)  # Use np.min instead of min\n",
    "daily_source_infected_cases = np.zeros(time_limit)\n",
    "for i, np_source_confirmed_date in enumerate(np_source_confirmed_dates):\n",
    "    # Convert timedelta to integer days\n",
    "    index = int((np_source_confirmed_date - min_confirmed_date) / np.timedelta64(1, 'D'))\n",
    "    if 0 <= index < time_limit:  # Add bounds check\n",
    "        daily_source_infected_cases[index] += 1  # Increment the day's count, not the sequence position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean of total confirmed cases: ', np.mean(np.sum(daily_confirmed_cases_matrix, axis=1)))\n",
    "print('Median of total confirmed cases: ', np.median(np.sum(daily_confirmed_cases_matrix, axis=1)))\n",
    "print('Number of source cases: ', len(sources_confirmed_dates))\n",
    "print('Total number of TW local confirmed cases (summary data): ', sum(daily_tw_local_confirmed_cases))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimize the day shift for each simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_and_pad(array1, array2, time_shift):\n",
    "    array1_padded = np.pad(array1, (time_shift, 0), 'constant', constant_values=0)\n",
    "    array2_padded = np.pad(array2, (0, time_shift), 'constant', constant_values=array2[-1])\n",
    "\n",
    "    return (array1_padded, array2_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_time_shift(dates, cumulative_cases, cumulative_simulated_cases):\n",
    "    \"\"\"\n",
    "    Find the optimal time shift between two arrays using a modified bisection search.\n",
    "    Returns the time shift that minimizes the mean squared error.\n",
    "    \"\"\"\n",
    "    # Apply bisection\n",
    "    time_shift_L = 0\n",
    "    time_shift_R = 57\n",
    "    # time_shift_R = 77\n",
    "    \n",
    "    while time_shift_R - time_shift_L > 1:\n",
    "        time_shift_M = (time_shift_L + time_shift_R) // 2\n",
    "        \n",
    "        # Compute losses for left, middle, and right points\n",
    "        L_cumulative_cases, L_shifted = shift_and_pad(cumulative_cases, cumulative_simulated_cases, time_shift_L)\n",
    "        M_cumulative_cases, M_shifted = shift_and_pad(cumulative_cases, cumulative_simulated_cases, time_shift_M)\n",
    "        R_cumulative_cases, R_shifted = shift_and_pad(cumulative_cases, cumulative_simulated_cases, time_shift_R)\n",
    "        \n",
    "        L_loss = mean_squared_error(L_cumulative_cases, L_shifted)\n",
    "        M_loss = mean_squared_error(M_cumulative_cases, M_shifted)\n",
    "        R_loss = mean_squared_error(R_cumulative_cases, R_shifted)\n",
    "        \n",
    "        # Update search interval - this assumes a unimodal error function\n",
    "        if L_loss < M_loss:\n",
    "            time_shift_R = time_shift_M\n",
    "        elif R_loss < M_loss:\n",
    "            time_shift_L = time_shift_M\n",
    "        else:\n",
    "            # If middle is the best, narrow from both sides\n",
    "            time_shift_L = (time_shift_L + time_shift_M) // 2\n",
    "            time_shift_R = (time_shift_M + time_shift_R) // 2\n",
    "    shifted_dates = np.arange(dates[0]-time_shift_L, dates[-1]+1)\n",
    "\n",
    "    return (time_shift_M, shifted_dates, M_cumulative_cases, M_shifted, M_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean value and 95% confidence interval for both daily and cumulative\n",
    "\n",
    "# Daily confirmed cases\n",
    "mean_daily_confirmed_cases = np.mean(daily_confirmed_cases_matrix, axis=0)\n",
    "\n",
    "# Daily deaths\n",
    "mean_daily_deaths = np.mean(daily_deaths_matrix, axis=0)\n",
    "\n",
    "# Cumulative confirmed cases\n",
    "cumulative_confirmed_cases_matrix = np.cumsum(daily_confirmed_cases_matrix, axis=1)\n",
    "mean_cumulative_confirmed_cases = np.mean(cumulative_confirmed_cases_matrix, axis=0)\n",
    "\n",
    "# Cumulative deaths\n",
    "cumulative_deaths_matrix = np.cumsum(daily_deaths_matrix, axis=1)\n",
    "mean_cumulative_deaths = np.mean(cumulative_deaths_matrix, axis=0)\n",
    "\n",
    "# Cumulative Taiwan data\n",
    "cumulative_tw_local_confirmed_cases = np.cumsum(daily_tw_local_confirmed_cases)\n",
    "cumulative_tw_local_death = np.cumsum(daily_tw_local_deaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize time shift\n",
    "optimal_t_shift, shifted_dates, shifted_cumulative_tw_local_confirmed_cases, shifted_mean_cumulative_confirmed_cases, mse_loss = optimize_time_shift(daily_tw_date_time, cumulative_tw_local_confirmed_cases, mean_cumulative_confirmed_cases)\n",
    "print(f\"Optimal time shift: {optimal_t_shift} days\")\n",
    "print(\"MSE loss: \", mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the case matrix and get the mean and the 95 CI.\n",
    "\n",
    "# Daily confirmed cases\n",
    "shifted_daily_confirmed_cases_matrix = np.pad(daily_confirmed_cases_matrix, ((0, 0), (0, optimal_t_shift)), 'constant', constant_values=0)\n",
    "shifted_mean_daily_confirmed_cases = np.mean(shifted_daily_confirmed_cases_matrix, axis=0)\n",
    "shifted_lb_daily_confirmed_cases = np.percentile(shifted_daily_confirmed_cases_matrix, 2.5, axis=0)\n",
    "shifted_ub_daily_confirmed_cases = np.percentile(shifted_daily_confirmed_cases_matrix, 97.5, axis=0)\n",
    "\n",
    "# Daily deaths\n",
    "shifted_daily_deaths_matrix = np.pad(daily_deaths_matrix, ((0, 0), (optimal_t_shift, 0)), 'constant', constant_values=0)\n",
    "shifted_mean_daily_deaths = np.mean(shifted_daily_deaths_matrix, axis=0)\n",
    "shifted_lb_daily_deaths = np.percentile(shifted_daily_deaths_matrix, 2.5, axis=0)\n",
    "shifted_ub_daily_deaths = np.percentile(shifted_daily_deaths_matrix, 97.5, axis=0)\n",
    "\n",
    "# Cumulative confirmed cases\n",
    "shifted_cumulative_confirmed_cases_matrix = np.cumsum(shifted_daily_confirmed_cases_matrix, axis=1)\n",
    "shifted_mean_cumulative_confirmed_cases = np.mean(shifted_cumulative_confirmed_cases_matrix, axis=0)\n",
    "shifted_lb_cumulative_confirmed_cases = np.percentile(shifted_cumulative_confirmed_cases_matrix, 2.5, axis=0)\n",
    "shifted_ub_cumulative_confirmed_cases = np.percentile(shifted_cumulative_confirmed_cases_matrix, 97.5, axis=0)\n",
    "\n",
    "# Cumulative deaths\n",
    "shifted_cumulative_deaths_matrix = np.cumsum(shifted_daily_deaths_matrix, axis=1)\n",
    "shifted_mean_cumulative_deaths = np.mean(shifted_cumulative_deaths_matrix, axis=0)\n",
    "shifted_lb_cumulative_deaths = np.percentile(shifted_cumulative_deaths_matrix, 2.5, axis=0) \n",
    "shifted_ub_cumulative_deaths = np.percentile(shifted_cumulative_deaths_matrix, 97.5, axis=0)\n",
    "\n",
    "# Shift Taiwan data\n",
    "shifted_daily_tw_local_confirmed_cases = np.pad(daily_tw_local_confirmed_cases, (optimal_t_shift, 0), 'constant', constant_values=0)\n",
    "shifted_daily_tw_local_deaths = np.pad(daily_tw_local_deaths, (optimal_t_shift, 0), 'constant', constant_values=0)\n",
    "shifted_cumulative_tw_local_deaths = np.cumsum(shifted_daily_tw_local_deaths)\n",
    "\n",
    "# Shift the source infection cases\n",
    "shifted_daily_source_infected_cases = np.pad(daily_source_infected_cases, (0, optimal_t_shift), 'constant', constant_values=0)\n",
    "shifted_cumulative_source_infected_cases = np.cumsum(shifted_daily_source_infected_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Calculate GOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gof(actual, predicted, t_type='daily'):\n",
    "    if t_type == 'daily':\n",
    "        pass\n",
    "    elif t_type == 'weekly':\n",
    "        window_size = 7\n",
    "        actual = np.convolve(actual, np.ones(window_size)/window_size, mode='same')\n",
    "        predicted = np.convolve(predicted, np.ones(window_size)/window_size, mode='same')\n",
    "    elif t_type == 'monthly':\n",
    "        window_size = 31 # odds number\n",
    "        actual = np.convolve(actual, np.ones(window_size)/window_size, mode='same')\n",
    "        predicted = np.convolve(predicted, np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "\n",
    "    cumulative_actual = np.cumsum(actual)\n",
    "    cumulative_predicted = np.cumsum(predicted)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    mae_cumsum = mean_absolute_error(cumulative_actual, cumulative_predicted)\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    mse_cumsum = mean_squared_error(cumulative_actual, cumulative_predicted)\n",
    "    rmse = root_mean_squared_error(actual, predicted)\n",
    "    rmse_cumsum = root_mean_squared_error(cumulative_actual, cumulative_predicted)\n",
    "    nae = np.mean(np.abs(actual-predicted)/(actual+predicted))\n",
    "    nae_cumsum = np.mean(np.abs(cumulative_actual-cumulative_predicted)/(cumulative_actual+cumulative_predicted))\n",
    "    mape = mean_absolute_percentage_error(actual, predicted)\n",
    "    mape_cumsum = mean_absolute_percentage_error(cumulative_actual, cumulative_predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    r2_cumsum = r2_score(cumulative_actual, cumulative_predicted)\n",
    "\n",
    "\n",
    "    return mae, mae_cumsum, mse, mse_cumsum, rmse, rmse_cumsum, nae, nae_cumsum, mape, mape_cumsum, r2, r2_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First confirmed case date to the last confirmed case date\n",
    "# start_date = np.datetime64('2020-01-27')\n",
    "\n",
    "# The first date of the first simulated confirmed case\n",
    "start_date = shifted_dates[0]\n",
    "start_index = np.where(shifted_dates == start_date)[0][0]\n",
    "# end_date = np.datetime64('2020-08-01')\n",
    "end_date = start_date + 28*6\n",
    "end_index = np.where(shifted_dates == end_date)[0][0]\n",
    "target_time_range_indices = range(start_index, end_index)\n",
    "gof_dates = shifted_dates[target_time_range_indices]\n",
    "\n",
    "# Select appropriate actual data\n",
    "actual_confirmed_cases = shifted_daily_tw_local_confirmed_cases[target_time_range_indices]\n",
    "cumulative_actual_confirmed_cases = shifted_cumulative_tw_local_confirmed_cases[target_time_range_indices]\n",
    "actual_deaths = shifted_daily_tw_local_deaths[target_time_range_indices]\n",
    "cumulative_actual_deaths = shifted_cumulative_tw_local_deaths[target_time_range_indices]\n",
    "\n",
    "# Select appropriate predicted mean data\n",
    "predicted_confirmed_cases = shifted_mean_daily_confirmed_cases[target_time_range_indices]\n",
    "cumulative_predicted_confirmed_cases = shifted_mean_cumulative_confirmed_cases[target_time_range_indices]\n",
    "predicted_deaths = shifted_mean_daily_deaths[target_time_range_indices]\n",
    "cumulative_predicted_deaths = shifted_mean_cumulative_deaths[target_time_range_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select appropriate predicted data matrix\n",
    "predicted_confirmed_cases_matrix = shifted_daily_confirmed_cases_matrix[:, target_time_range_indices]\n",
    "cumulative_predicted_confirmed_cases_matrix = shifted_cumulative_confirmed_cases_matrix[:, target_time_range_indices]\n",
    "predicted_deaths_matrix = shifted_daily_deaths_matrix[:, target_time_range_indices]\n",
    "cumulative_predicted_deaths_matrix = shifted_cumulative_deaths_matrix[:, target_time_range_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(actual_confirmed_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data types, time periods, and metrics\n",
    "data_types = ['Confirmed', 'Deaths']\n",
    "t_types = ['daily', 'weekly', 'monthly']\n",
    "metrics = ['MAE', 'MAE (Cumulative)', 'MSE', 'MSE (Cumulative)', 'RMSE', 'RMSE (Cumulative)', 'NAE', 'NAE (Cumulative)', \n",
    "           'MAPE', 'MAPE (Cumulative)', 'R2', 'R2 (Cumulative)']\n",
    "\n",
    "# Create dictionary to store results\n",
    "results_dict = {}\n",
    "\n",
    "# Loop through each combination of data type and time period\n",
    "for t_type in t_types:\n",
    "    for data_type in data_types:\n",
    "        row_name = f\"{t_type} {data_type}\"\n",
    "        results_dict[row_name] = {}  # Create a nested dictionary for each row\n",
    "        \n",
    "        # Select appropriate actual and predicted data\n",
    "        if data_type == 'Confirmed':\n",
    "            actual = actual_confirmed_cases\n",
    "            predicted_matrix = predicted_confirmed_cases_matrix\n",
    "        else:  # Deaths\n",
    "            actual = actual_deaths\n",
    "            predicted_matrix = predicted_deaths_matrix\n",
    "        \n",
    "        # Calculate goodness of fit with the updated function that returns all metrics\n",
    "        maes = []\n",
    "        maes_cumsum = []\n",
    "        mses = []\n",
    "        mses_cumsum = []\n",
    "        rmses = []\n",
    "        rmses_cumsum = []\n",
    "        r2s = []\n",
    "        r2s_cumsum = []\n",
    "        for i in range(predicted_matrix.shape[0]):\n",
    "            predicted = predicted_matrix[i, :]\n",
    "            mae, mae_cumsum, mse, mse_cumsum, rmse, rmse_cumsum, nae, nae_cumsum, mape, mape_cumsum, r2, r2_cumsum = gof(actual, predicted, t_type=t_type)\n",
    "            maes.append(mae)\n",
    "            maes_cumsum.append(mae_cumsum)\n",
    "            mses.append(mse)\n",
    "            mses_cumsum.append(mse_cumsum)\n",
    "            rmses.append(rmse)\n",
    "            rmses_cumsum.append(rmse_cumsum)\n",
    "            r2s.append(r2)\n",
    "            r2s_cumsum.append(r2_cumsum)\n",
    "\n",
    "        mae_mean = np.mean(maes)\n",
    "        mae_ub = np.percentile(maes, 97.5)\n",
    "        mae_lb = np.percentile(maes, 2.5)\n",
    "        mae_cumsum_mean = np.mean(maes_cumsum)\n",
    "        mae_cumsum_ub = np.percentile(maes_cumsum, 97.5)\n",
    "        mae_cumsum_lb = np.percentile(maes_cumsum, 2.5)\n",
    "        mse_mean = np.mean(mses)\n",
    "        mse_ub = np.percentile(mses, 97.5)\n",
    "        mse_lb = np.percentile(mses, 2.5)\n",
    "        mse_cumsum_mean = np.mean(mses_cumsum)\n",
    "        mse_cumsum_ub = np.percentile(mses_cumsum, 97.5)\n",
    "        mse_cumsum_lb = np.percentile(mses_cumsum, 2.5)\n",
    "        rmse_mean = np.mean(rmses)\n",
    "        rmse_ub = np.percentile(rmses, 97.5)\n",
    "        rmse_lb = np.percentile(rmses, 2.5)\n",
    "        rmse_cumsum_mean = np.mean(rmses_cumsum)\n",
    "        rmse_cumsum_ub = np.percentile(rmses_cumsum, 97.5)\n",
    "        rmse_cumsum_lb = np.percentile(rmses_cumsum, 2.5)\n",
    "        r2_mean = np.mean(r2s)\n",
    "        r2_ub = np.percentile(r2s, 97.5)\n",
    "        r2_lb = np.percentile(r2s, 2.5)\n",
    "        r2_cumsum_mean = np.mean(r2s_cumsum)\n",
    "        r2_cumsum_ub = np.percentile(r2s_cumsum, 97.5)\n",
    "        r2_cumsum_lb = np.percentile(r2s_cumsum, 2.5)\n",
    "        \n",
    "        # Store results with metrics as column names\n",
    "        results_dict[row_name]['MAE'] = mae_mean\n",
    "        results_dict[row_name]['MAE ub'] = mae_ub\n",
    "        results_dict[row_name]['MAE lb'] = mae_lb\n",
    "        results_dict[row_name]['MAE (Cumulative)'] = mae_cumsum_mean\n",
    "        results_dict[row_name]['MAE (Cumulative) ub'] = mae_cumsum_ub\n",
    "        results_dict[row_name]['MAE (Cumulative) lb'] = mae_cumsum_lb\n",
    "        results_dict[row_name]['MSE'] = mse_mean\n",
    "        results_dict[row_name]['MSE ub'] = mse_ub\n",
    "        results_dict[row_name]['MSE lb'] = mse_lb\n",
    "        results_dict[row_name]['MSE (Cumulative)'] = mse_cumsum_mean\n",
    "        results_dict[row_name]['MSE (Cumulative) ub'] = mse_cumsum_ub\n",
    "        results_dict[row_name]['MSE (Cumulative) lb'] = mse_cumsum_lb\n",
    "        results_dict[row_name]['RMSE'] = rmse_mean\n",
    "        results_dict[row_name]['RMSE ub'] = rmse_ub\n",
    "        results_dict[row_name]['RMSE lb'] = rmse_lb\n",
    "        results_dict[row_name]['RMSE (Cumulative)'] = rmse_cumsum_mean\n",
    "        results_dict[row_name]['RMSE (Cumulative) ub'] = rmse_cumsum_ub\n",
    "        results_dict[row_name]['RMSE (Cumulative) lb'] = rmse_cumsum_lb\n",
    "        results_dict[row_name]['R2'] = r2_mean\n",
    "        results_dict[row_name]['R2 ub'] = r2_ub\n",
    "        results_dict[row_name]['R2 lb'] = r2_lb\n",
    "        results_dict[row_name]['R2 (Cumulative)'] = r2_cumsum_mean\n",
    "        results_dict[row_name]['R2 (Cumulative) ub'] = r2_cumsum_ub\n",
    "        results_dict[row_name]['R2 (Cumulative) lb'] = r2_cumsum_lb\n",
    "\n",
    "# Create DataFrame with data types and time periods as rows, metrics as columns\n",
    "results_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "\n",
    "# Add a column for the row names (can be useful for further processing)\n",
    "results_df = results_df.reset_index().rename(columns={'index': ''})\n",
    "\n",
    "# Round all numeric values to 3 decimal places\n",
    "numeric_columns = results_df.columns[1:]  # All columns except 'Data Type'\n",
    "results_df[numeric_columns] = results_df[numeric_columns].round(2)\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('goodness_of_fit_results.csv', index=False)\n",
    "\n",
    "# Display the table\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shift result plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "gof_cumulative_source_infected_cases = shifted_cumulative_source_infected_cases[target_time_range_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 95% confidence interval\n",
    "gof_cumulative_confirmed_cases_lb = shifted_lb_cumulative_confirmed_cases[target_time_range_indices]\n",
    "gof_cumulative_confirmed_cases_ub = shifted_ub_cumulative_confirmed_cases[target_time_range_indices]\n",
    "gof_cumulative_deaths_lb = shifted_lb_cumulative_deaths[target_time_range_indices]\n",
    "gof_cumulative_deaths_ub = shifted_ub_cumulative_deaths[target_time_range_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_cumulative_confirmed_cases_dict = {0.025: np.quantile(shifted_cumulative_confirmed_cases_matrix, 0.025, axis=0)[target_time_range_indices],\n",
    "                      0.975: np.quantile(shifted_cumulative_confirmed_cases_matrix, 0.975, axis=0)[target_time_range_indices]}\n",
    "cumulative_confirmed_cases_wis_score, sharpness, calibration = scoring.weighted_interval_score(cumulative_actual_confirmed_cases, alphas=[0.05], \n",
    "                              q_dict=quantile_cumulative_confirmed_cases_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_cumulative_deaths_dict = {0.025: np.quantile(shifted_cumulative_deaths_matrix, 0.025, axis=0)[target_time_range_indices],\n",
    "                      0.975: np.quantile(shifted_cumulative_deaths_matrix, 0.975, axis=0)[target_time_range_indices]}\n",
    "cumulative_deaths_wis_score, death_sharpness, death_calibration = scoring.weighted_interval_score(np.cumsum(actual_deaths), alphas=[0.05], \n",
    "                              q_dict=quantile_cumulative_deaths_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(10, 9), gridspec_kw={'height_ratios': [2, 1]})\n",
    "\n",
    "# Daily confirmed cases\n",
    "axs[0].plot(gof_dates, cumulative_actual_confirmed_cases, ':', color=current_palette[0])\n",
    "axs[0].plot(gof_dates, cumulative_actual_deaths, ':', color=current_palette[2])\n",
    "# Create a mask for points where there's an increase in confirmed cases\n",
    "is_case_increase = np.concatenate([[True], np.diff(cumulative_actual_confirmed_cases) > 0])\n",
    "# Create a mask for points where there's an increase in deaths\n",
    "is_death_increase = np.concatenate([[True], np.diff(cumulative_actual_deaths) > 0])\n",
    "axs[0].plot(gof_dates[is_case_increase], cumulative_actual_confirmed_cases[is_case_increase], \n",
    "         '*', color=current_palette[0], markersize=8, label='Observed local confirmed cases')\n",
    "axs[0].plot(gof_dates[is_death_increase], cumulative_actual_deaths[is_death_increase], \n",
    "         'X', color=current_palette[2], markersize=8, label='Observed local deaths')\n",
    "\n",
    "# CovSyn daily confirmed cases\n",
    "axs[0].plot(gof_dates, cumulative_predicted_confirmed_cases, color=current_palette[0], label='CovSyn local confirmed cases')\n",
    "axs[0].fill_between(gof_dates, gof_cumulative_confirmed_cases_lb, gof_cumulative_confirmed_cases_ub, color=current_palette[0], alpha=0.3, label='95% CI for CovSyn cases')\n",
    "\n",
    "# CovSyn daily death cases\n",
    "axs[0].plot(gof_dates, cumulative_predicted_deaths, color=current_palette[2], label='CovSyn local deaths')\n",
    "axs[0].fill_between(gof_dates, gof_cumulative_deaths_lb, gof_cumulative_deaths_ub, color=current_palette[2], alpha=0.3, label='95% CI for CovSyn deaths')\n",
    "\n",
    "# Source infected cases\n",
    "axs[0].plot(gof_dates, gof_cumulative_source_infected_cases, '--', color=current_palette[3], label='Source infected cases')\n",
    "\n",
    "# Generate exact dates for major ticks every 28 days\n",
    "major_tick_dates = []\n",
    "current_date = start_date\n",
    "plot_end_date = end_date\n",
    "while current_date <= plot_end_date:\n",
    "    major_tick_dates.append(current_date)\n",
    "    current_date = current_date + timedelta(days=28)\n",
    "\n",
    "# Generate weekly minor ticks between each major tick\n",
    "minor_tick_dates = []\n",
    "for i in range(len(major_tick_dates)-1):\n",
    "    current = major_tick_dates[i] + timedelta(days=7)\n",
    "    while current < major_tick_dates[i+1]:\n",
    "        minor_tick_dates.append(current)\n",
    "        current = current + timedelta(days=7)\n",
    "\n",
    "# Convert dates to matplotlib's ordinal format\n",
    "major_tick_locations = [mdates.date2num(d) for d in major_tick_dates]\n",
    "minor_tick_locations = [mdates.date2num(d) for d in minor_tick_dates]\n",
    "\n",
    "# Configure first subplot (axs[0])\n",
    "axs[0].legend(numpoints=1, loc='best')\n",
    "\n",
    "# Set major ticks but hide labels\n",
    "axs[0].set_xticks(major_tick_locations)\n",
    "# Don't set DateFormatter here - that would override our empty labels\n",
    "axs[0].tick_params(axis='x', which='major', length=6)  # Make ticks visible\n",
    "\n",
    "# Important: After setting ticks, hide the labels by setting them to empty strings\n",
    "axs[0].set_xticklabels(['' for _ in range(len(major_tick_locations))])\n",
    "\n",
    "# Set minor ticks\n",
    "axs[0].set_xticks(minor_tick_locations, minor=True)\n",
    "axs[0].tick_params(axis='x', which='minor', bottom=True)\n",
    "\n",
    "# Add grid aligned with ticks\n",
    "axs[0].grid(True, which='major', axis='both', linestyle='dotted', color='gray', alpha=0.7)\n",
    "\n",
    "axs[0].set_ylabel('Number of cases')\n",
    "axs[0].set_xlim(start_date - timedelta(days=2), plot_end_date + timedelta(days=2))\n",
    "# axs[0].set_ylim([0, 4.1])\n",
    "\n",
    "# WIS plot for second subplot (axs[1])\n",
    "axs[1].plot(gof_dates, cumulative_confirmed_cases_wis_score, color=current_palette[0], label='Confirmed cases')\n",
    "axs[1].plot(gof_dates, cumulative_deaths_wis_score, color=current_palette[2], label='Deaths')\n",
    "\n",
    "# Configure second subplot (axs[1]) with same x-axis settings\n",
    "axs[1].set_xticks(major_tick_locations)\n",
    "axs[1].xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))\n",
    "axs[1].tick_params(axis='x', rotation=30)\n",
    "\n",
    "# Set minor ticks\n",
    "axs[1].set_xticks(minor_tick_locations, minor=True)\n",
    "axs[1].tick_params(axis='x', which='minor', bottom=True)\n",
    "\n",
    "# Add grid aligned with ticks\n",
    "axs[1].grid(True, which='major', axis='both', linestyle='dotted', color='gray', alpha=0.7)\n",
    "\n",
    "axs[1].set_ylabel('Weighted interval score')\n",
    "# axs[1].set_xlabel('Date')\n",
    "axs[1].set_xlim(start_date - timedelta(days=2), plot_end_date + timedelta(days=2))\n",
    "axs[1].legend()\n",
    "\n",
    "# Set horizontal alignment for tick labels - only for the second subplot\n",
    "plt.setp(axs[1].get_xticklabels(), ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "df = pd.DataFrame({'Dates': gof_dates, 'WIS':cumulative_confirmed_cases_wis_score, \n",
    "                   'WIS death': cumulative_deaths_wis_score, 'Actual data': cumulative_actual_confirmed_cases})\n",
    "# df.to_csv('wis_score.csv')\n",
    "# plt.savefig(\"RW2025_Covsyn_population_data_wis.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(9.5, 6))\n",
    "\n",
    "# Daily confirmed cases\n",
    "axs.plot(gof_dates, cumulative_actual_confirmed_cases, ':', color=current_palette[0])\n",
    "axs.plot(gof_dates, cumulative_actual_deaths, ':', color=current_palette[2])\n",
    "# Create a mask for points where there's an increase in confirmed cases\n",
    "is_case_increase = np.concatenate([[True], np.diff(cumulative_actual_confirmed_cases) > 0])\n",
    "# Create a mask for points where there's an increase in deaths\n",
    "is_death_increase = np.concatenate([[True], np.diff(cumulative_actual_deaths) > 0])\n",
    "axs.plot(gof_dates[is_case_increase], cumulative_actual_confirmed_cases[is_case_increase], \n",
    "         '*', color=current_palette[0], markersize=8, label='Observed local confirmed cases')\n",
    "axs.plot(gof_dates[is_death_increase], cumulative_actual_deaths[is_death_increase], \n",
    "         'X', color=current_palette[2], markersize=8, label='Observed local deaths')\n",
    "\n",
    "# CovSyn daily confirmed cases\n",
    "axs.plot(gof_dates, cumulative_predicted_confirmed_cases, color=current_palette[0], label='CovSyn local confirmed cases')\n",
    "axs.fill_between(gof_dates, gof_cumulative_confirmed_cases_lb, gof_cumulative_confirmed_cases_ub, color=current_palette[0], alpha=0.3, label='95% CI for CovSyn cases')\n",
    "\n",
    "# CovSyn daily death cases\n",
    "axs.plot(gof_dates, cumulative_predicted_deaths, color=current_palette[2], label='CovSyn local deaths')\n",
    "axs.fill_between(gof_dates, gof_cumulative_deaths_lb, gof_cumulative_deaths_ub, color=current_palette[2], alpha=0.3, label='95% CI for CovSyn deaths')\n",
    "\n",
    "# Plot the soure infected cases\n",
    "axs.plot(gof_dates, gof_cumulative_source_infected_cases, '--', color=current_palette[3], label='Source infected cases')\n",
    "\n",
    "# Generate exact dates for major ticks every 28 days\n",
    "major_tick_dates = []\n",
    "current_date = start_date\n",
    "# plot_end_date = np.datetime64('2020-08-10')\n",
    "plot_end_date = end_date\n",
    "while current_date <= plot_end_date:\n",
    "    major_tick_dates.append(current_date)\n",
    "    current_date = current_date + timedelta(days=28)\n",
    "\n",
    "# Generate weekly minor ticks between each major tick\n",
    "minor_tick_dates = []\n",
    "for i in range(len(major_tick_dates)-1):\n",
    "    current = major_tick_dates[i] + timedelta(days=7)\n",
    "    while current < major_tick_dates[i+1]:\n",
    "        minor_tick_dates.append(current)\n",
    "        current = current + timedelta(days=7)\n",
    "\n",
    "# Convert dates to matplotlib's ordinal format\n",
    "major_tick_locations = [mdates.date2num(d) for d in major_tick_dates]\n",
    "minor_tick_locations = [mdates.date2num(d) for d in minor_tick_dates]\n",
    "\n",
    "# Configure first subplot (axs)\n",
    "axs.legend(numpoints=1, loc='best')\n",
    "\n",
    "# Configure second subplot (axs[1]) with same x-axis settings\n",
    "axs.set_xticks(major_tick_locations)\n",
    "axs.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))\n",
    "axs.tick_params(axis='x', rotation=30)\n",
    "\n",
    "# Important: After setting ticks, hide the labels by setting them to empty strings\n",
    "# axs.set_xticklabels(['' for _ in range(len(major_tick_locations))])\n",
    "\n",
    "# Set minor ticks\n",
    "axs.set_xticks(minor_tick_locations, minor=True)\n",
    "axs.tick_params(axis='x', which='minor', bottom=True)\n",
    "\n",
    "plt.setp(axs.get_xticklabels(), ha='right')\n",
    "\n",
    "# Add grid aligned with ticks\n",
    "axs.grid(True, which='major', axis='both', linestyle='dotted', color='gray', alpha=0.7)\n",
    "\n",
    "axs.set_ylabel('Cumulative number of cases')\n",
    "axs.set_xlim(start_date - timedelta(days=2), plot_end_date + timedelta(days=2))\n",
    "# axs.set_yscale('log')\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"RW2025_Covsyn_population_data.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covsyn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
